{"metadata":{"colab":{"name":"MTL_Run_S.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome to the HydraNet Workshop 🐸🐸🐸\nIn this workshop, you're going to learn how to build a Neural Network that has:\n* Input: **a monocular RGB Image**\n* Output: **a Depth Map**, and **a Segmentation Map**\n\nA single model, two different outputs. For that, out model will need to use a principle called Multi Task Learning.<p>","metadata":{"id":"20Fk1u9m9xfl"}},{"cell_type":"markdown","source":"# 1 - Imports","metadata":{"id":"5Ix2A28T-ZT_"}},{"cell_type":"code","source":"!pip install -U tensorflow","metadata":{"execution":{"iopub.status.busy":"2022-12-17T01:33:07.306237Z","iopub.execute_input":"2022-12-17T01:33:07.306715Z","iopub.status.idle":"2022-12-17T01:34:48.473444Z","shell.execute_reply.started":"2022-12-17T01:33:07.306618Z","shell.execute_reply":"2022-12-17T01:34:48.472262Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.6.4)\nCollecting tensorflow\n  Downloading tensorflow-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.21.6)\nCollecting tensorflow-estimator<2.12,>=2.11.0\n  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tensorboard<2.12,>=2.11\n  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting libclang>=13.0.0\n  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12.1)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (4.4.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow) (59.8.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.7.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.15.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.43.0)\nCollecting flatbuffers>=2.0\n  Downloading flatbuffers-22.12.6-py2.py3-none-any.whl (26 kB)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.19.4)\nCollecting keras<2.12,>=2.11.0\n  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n  Downloading tensorflow_io_gcs_filesystem-0.28.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\nCollecting absl-py>=1.0.0\n  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.3.7)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.35.0)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.2)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->tensorflow) (3.0.9)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.2.4)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.8)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.13.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.0)\nInstalling collected packages: libclang, flatbuffers, tensorflow-io-gcs-filesystem, tensorflow-estimator, keras, absl-py, tensorboard, tensorflow\n  Attempting uninstall: flatbuffers\n    Found existing installation: flatbuffers 1.12\n    Uninstalling flatbuffers-1.12:\n      Successfully uninstalled flatbuffers-1.12\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.6.0\n    Uninstalling tensorflow-estimator-2.6.0:\n      Successfully uninstalled tensorflow-estimator-2.6.0\n  Attempting uninstall: keras\n    Found existing installation: keras 2.6.0\n    Uninstalling keras-2.6.0:\n      Successfully uninstalled keras-2.6.0\n  Attempting uninstall: absl-py\n    Found existing installation: absl-py 0.15.0\n    Uninstalling absl-py-0.15.0:\n      Successfully uninstalled absl-py-0.15.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.10.1\n    Uninstalling tensorboard-2.10.1:\n      Successfully uninstalled tensorboard-2.10.1\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.6.4\n    Uninstalling tensorflow-2.6.4:\n      Successfully uninstalled tensorflow-2.6.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntfx-bsl 1.9.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\ntensorflow-transform 1.9.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.11.0 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 2.11.0 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.28.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed absl-py-1.3.0 flatbuffers-22.12.6 keras-2.11.0 libclang-14.0.6 tensorboard-2.11.0 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.28.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!wget https://hydranets-data.s3.eu-west-3.amazonaws.com/hydranets-data.zip && unzip -q hydranets-data.zip && mv hydranets-data/* . && rm hydranets-data.zip && rm -rf hydranets-data","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wnk8xflh-RfW","outputId":"651894ce-c17f-4b40-8b22-fdbfca19c527","execution":{"iopub.status.busy":"2022-12-17T01:34:48.475700Z","iopub.execute_input":"2022-12-17T01:34:48.476013Z","iopub.status.idle":"2022-12-17T01:35:03.114898Z","shell.execute_reply.started":"2022-12-17T01:34:48.475979Z","shell.execute_reply":"2022-12-17T01:35:03.113525Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2022-12-17 01:34:49--  https://hydranets-data.s3.eu-west-3.amazonaws.com/hydranets-data.zip\nResolving hydranets-data.s3.eu-west-3.amazonaws.com (hydranets-data.s3.eu-west-3.amazonaws.com)... 52.95.154.52\nConnecting to hydranets-data.s3.eu-west-3.amazonaws.com (hydranets-data.s3.eu-west-3.amazonaws.com)|52.95.154.52|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 110752264 (106M) [application/zip]\nSaving to: ‘hydranets-data.zip’\n\nhydranets-data.zip  100%[===================>] 105.62M  11.4MB/s    in 11s     \n\n2022-12-17 01:35:01 (9.53 MB/s) - ‘hydranets-data.zip’ saved [110752264/110752264]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nimport math","metadata":{"id":"szGHb1YQSyJ_","execution":{"iopub.status.busy":"2022-12-17T01:35:03.116938Z","iopub.execute_input":"2022-12-17T01:35:03.117388Z","iopub.status.idle":"2022-12-17T01:35:07.815264Z","shell.execute_reply.started":"2022-12-17T01:35:03.117341Z","shell.execute_reply":"2022-12-17T01:35:07.814291Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2022-12-17 01:35:03.320040: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-17 01:35:03.499784: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n2022-12-17 01:35:03.499832: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2022-12-17 01:35:04.782317: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n2022-12-17 01:35:04.782516: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n2022-12-17 01:35:04.782532: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","output_type":"stream"}]},{"cell_type":"code","source":"tf.keras.Sequential([tf.keras.layers.Conv2D(12, kernel_size=3, strides=2, input_shape=(32,32,1)),\ntf.keras.layers.GlobalMaxPool2D(),\ntf.keras.layers.Dense(1)]).summary()\n\nlayers = []\nfor i, u in enumerate([10,10,1]) :\n    if i == 0 :\n        layers.append(tf.keras.layers.Dense(u, input_shape=(5,)))\n    else :\n        layers.append(tf.keras.layers.Dense(u))\ntf.keras.Sequential(layers).summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-17T01:36:32.205023Z","iopub.execute_input":"2022-12-17T01:36:32.205493Z","iopub.status.idle":"2022-12-17T01:36:32.292779Z","shell.execute_reply.started":"2022-12-17T01:36:32.205440Z","shell.execute_reply":"2022-12-17T01:36:32.291546Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_4 (Dense)             (None, 10)                60        \n                                                                 \n dense_5 (Dense)             (None, 10)                110       \n                                                                 \n dense_6 (Dense)             (None, 1)                 11        \n                                                                 \n=================================================================\nTotal params: 181\nTrainable params: 181\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2 — Creating the HydraNet\nWe now have 2 DataLoaders: one for training, and one for validation/test. <p>\n\nIn the next step, we're going to define our model, following the paper [Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations](https://arxiv.org/pdf/1809.04766.pdf) —— If you haven't read it yet, now is the time.<p>\n\nA Note — This notebook has been adapted from DrSleep, a researcher named Vladimir, who authorized me to adapt it for education purposes. [Here's the notebook I'm refering to](https://github.com/DrSleep/multi-task-refinenet/blob/master/src/notebooks/ExpNYUDKITTI_joint.ipynb/).\n\n<p>\n\n> ![](https://d3i71xaburhd42.cloudfront.net/435d4b5c30f10753d277848a17baddebd98d3c31/2-Figure1-1.png)\n\nOur model takes an input RGB image, make it go through an encoder, a lightweight refinenet decoder, and then has 2 heads, one for each task.<p>\nThings to note:\n* The only **convolutions** we'll need will be 3x3 and 1x1\n* We also need a **MaxPooling 5x5**\n* **CRP-Blocks** are implemented as Skip-Connection Operations\n* **Each Head is made of a 1x1 convolution followed by a 3x3 convolution**, only the data and the loss change there\n","metadata":{"id":"LuR-bk-13cX6"}},{"cell_type":"markdown","source":"## 2.1 — Create a HydraNet class","metadata":{"id":"Pf8671mIE_tU"}},{"cell_type":"markdown","source":"```\nS1\n    convbnrelu(32, k=3, s=2, padding=1, bias=False)\n    batchnorm()\n    relu6()\n    \nS2\n    \n```    ","metadata":{"execution":{"iopub.status.busy":"2022-12-17T01:53:05.397541Z","iopub.execute_input":"2022-12-17T01:53:05.398003Z","iopub.status.idle":"2022-12-17T01:53:05.405818Z","shell.execute_reply.started":"2022-12-17T01:53:05.397970Z","shell.execute_reply":"2022-12-17T01:53:05.404133Z"}}},{"cell_type":"code","source":"# cnn1 = tf.keras.Sequential([tf.keras.layers.Dense(20, input_shape=(5,)),\n#                            tf.keras.layers.Dense(20), \n#                            tf.keras.layers.Dense(10)])\n# cnn2 = tf.keras.Sequential([cnn1,\n#                            tf.keras.layers.Dense(1)])\n\n# cnn2.build(input_shape=(5,1))\n\n# cnn2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-17T02:27:26.308977Z","iopub.execute_input":"2022-12-17T02:27:26.309437Z","iopub.status.idle":"2022-12-17T02:27:26.315128Z","shell.execute_reply.started":"2022-12-17T02:27:26.309401Z","shell.execute_reply":"2022-12-17T02:27:26.314037Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"class HydraNet(tf.keras.Model):\n    def __init__(self):        \n        super().__init__() # Python 3\n        self.num_tasks = 2\n        self.num_classes = 6","metadata":{"id":"IwlTtYrR3vIA","execution":{"iopub.status.busy":"2022-12-17T02:28:13.235921Z","iopub.execute_input":"2022-12-17T02:28:13.236341Z","iopub.status.idle":"2022-12-17T02:28:13.242839Z","shell.execute_reply.started":"2022-12-17T02:28:13.236308Z","shell.execute_reply":"2022-12-17T02:28:13.241514Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"def define_mobilenet(self):\n    mobilenet_config = [[1, 16, 1, 1], # expansion rate, output channels, number of repeats, stride\n                    [6, 24, 2, 2],\n                    [6, 32, 3, 2],\n                    [6, 64, 4, 2],\n                    [6, 96, 3, 1],\n                    [6, 160, 3, 2],\n                    [6, 320, 1, 1],\n                    ]\n    in_channels = 32 # number of input channels\n    num_layers = len(mobilenet_config)\n    layer1 = convbnrelu(3, kernel_size=3, stride=2, groups=1, act=True)\n#     c_layer = 2\n#     for t,c,n,s in (mobilenet_config):\n#         layers = []\n#         for idx in range(n):\n#             layers.append(InvertedResidualBlock(c, expansion_factor=t, stride=s if idx == 0 else 1))\n#             in_channels = c\n#         c_layer += 1","metadata":{"execution":{"iopub.status.busy":"2022-12-17T02:30:38.821493Z","iopub.execute_input":"2022-12-17T02:30:38.821913Z","iopub.status.idle":"2022-12-17T02:30:38.829803Z","shell.execute_reply.started":"2022-12-17T02:30:38.821881Z","shell.execute_reply":"2022-12-17T02:30:38.828641Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 — Defining the Encoder: A MobileNetv2\n![](https://iq.opengenus.org/content/images/2020/11/conv_mobilenet_v2.jpg)","metadata":{"id":"025K4utrBqNy"}},{"cell_type":"code","source":"def conv3x3(in_channels, out_channels, stride=1, dilation=1, groups=1, bias=False):\n    \"\"\"3x3 Convolution: Depthwise: \n    https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n    \"\"\"\n    return tf.keras.layers.Conv2D(in_channels, out_channels, kernel_size=3, strides=stride, padding=dilation, dilation_rate=dilation, use_bias=bias, groups=groups)","metadata":{"id":"TpCvdaopBGqo","execution":{"iopub.status.busy":"2022-12-17T02:30:39.147182Z","iopub.execute_input":"2022-12-17T02:30:39.147604Z","iopub.status.idle":"2022-12-17T02:30:39.154673Z","shell.execute_reply.started":"2022-12-17T02:30:39.147572Z","shell.execute_reply":"2022-12-17T02:30:39.153515Z"},"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"code","source":"def conv1x1(in_channels, out_channels, stride=1, groups=1, bias=False,):\n    \"1x1 Convolution: Pointwise\"\n    return tf.keras.layers.Conv2D(in_channels, out_channels, kernel_size=1, strides=stride, padding=0, use_bias=bias, groups=groups)","metadata":{"id":"X26XhiFLBZD5","execution":{"iopub.status.busy":"2022-12-17T02:30:39.299725Z","iopub.execute_input":"2022-12-17T02:30:39.300135Z","iopub.status.idle":"2022-12-17T02:30:39.306620Z","shell.execute_reply.started":"2022-12-17T02:30:39.300102Z","shell.execute_reply":"2022-12-17T02:30:39.305328Z"},"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"code","source":"def batchnorm():\n    \"\"\"\n    https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n    \"\"\"\n    batch_norm =  tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1)\n    batch_norm.trainable = True\n    return batch_norm","metadata":{"id":"ytAJff2_QJym","execution":{"iopub.status.busy":"2022-12-17T02:30:39.504226Z","iopub.execute_input":"2022-12-17T02:30:39.504936Z","iopub.status.idle":"2022-12-17T02:30:39.510051Z","shell.execute_reply.started":"2022-12-17T02:30:39.504895Z","shell.execute_reply":"2022-12-17T02:30:39.508944Z"},"trusted":true},"execution_count":166,"outputs":[]},{"cell_type":"code","source":"def convbnrelu(out_channels)#, kernel_size, stride, groups, act):\n    \"conv-batchnorm-relu\"\n    padding = tf.constant([[int(kernel_size / 2), int(kernel_size / 2)], [int(kernel_size / 2),int(kernel_size / 2)]])\n    if act:\n        return tf.keras.Sequential(tf.keras.layers.Conv2D(out_channels, kernel_size=kernel_size,\n                                                          strides=stride, groups=groups, use_bias=False),\n                                   padding,\n                                   batchnorm(),\n                                   tf.keras.layers.ReLU(max_value=6))\n    else:\n        return tf.keras.Sequential(tf.keras.layers.Conv2D(out_channels, kernel_size=(kernel_size,kernel_size), \n                                                          strides=(stride,stride), groups=groups, use_bias=False),\n                                   padding,\n                                   batchnorm())","metadata":{"id":"vDJDvGiQSy2L","execution":{"iopub.status.busy":"2022-12-17T02:30:40.830565Z","iopub.execute_input":"2022-12-17T02:30:40.831151Z","iopub.status.idle":"2022-12-17T02:30:40.838877Z","shell.execute_reply.started":"2022-12-17T02:30:40.831118Z","shell.execute_reply":"2022-12-17T02:30:40.837893Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"code","source":"convbnrelu(3)#, kernel_size=3, stride=2, groups=1, act=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-17T02:31:59.096047Z","iopub.execute_input":"2022-12-17T02:31:59.096510Z","iopub.status.idle":"2022-12-17T02:31:59.122629Z","shell.execute_reply.started":"2022-12-17T02:31:59.096461Z","shell.execute_reply":"2022-12-17T02:31:59.121066Z"},"trusted":true},"execution_count":174,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/762783493.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconvbnrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, kernel_size=3, stride=2, groups=1, act=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: convbnrelu() missing 4 required positional arguments: 'kernel_size', 'stride', 'groups', and 'act'"],"ename":"TypeError","evalue":"convbnrelu() missing 4 required positional arguments: 'kernel_size', 'stride', 'groups', and 'act'","output_type":"error"}]},{"cell_type":"code","source":"# # num_filters = 8\n# # filter_size = 3\n# # pool_size = 2\n\n# print(tf.keras.Sequential([\n#     tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=(2,2), use_bias=False),\n#     batchnorm(),\n#     tf.keras.layers.Flatten(),\n#     tf.keras.layers.Dense(10, activation='softmax'),\n# ])\n# )","metadata":{"execution":{"iopub.status.busy":"2022-12-17T02:30:39.715435Z","iopub.execute_input":"2022-12-17T02:30:39.716680Z","iopub.status.idle":"2022-12-17T02:30:39.721785Z","shell.execute_reply.started":"2022-12-17T02:30:39.716629Z","shell.execute_reply":"2022-12-17T02:30:39.720938Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"# t = tf.constant([[1, 2, 3], [4, 5, 6]])\n# paddings = tf.constant([[int(3/2), int(3/2)], [int(3/2),int(3/2)]])\n# tf.pad(t, paddings, \"CONSTANT\")","metadata":{"execution":{"iopub.status.busy":"2022-12-17T02:30:40.108364Z","iopub.execute_input":"2022-12-17T02:30:40.108783Z","iopub.status.idle":"2022-12-17T02:30:40.114072Z","shell.execute_reply.started":"2022-12-17T02:30:40.108749Z","shell.execute_reply":"2022-12-17T02:30:40.112964Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"code","source":"# inp = tf.keras.Input((32, 32, 1)) # e.g. CIFAR10 images\n# custom_padded = tf.pad(inp, ((int(3/2.), int(3/2.)), (int(3/2.), int(3/2.)), (int(3/2.), int(3/2.)), (int(3/2.), int(3/2.))))\n# conv = tf.keras.layers.Conv2D(16, 3)(custom_padded)  # default padding is \"valid\"\n\n# model = tf.keras.Model(inp, conv)\n\n# # model.summary()\n# custom_padded","metadata":{"execution":{"iopub.status.busy":"2022-12-17T02:30:40.538451Z","iopub.execute_input":"2022-12-17T02:30:40.539393Z","iopub.status.idle":"2022-12-17T02:30:40.543658Z","shell.execute_reply.started":"2022-12-17T02:30:40.539358Z","shell.execute_reply":"2022-12-17T02:30:40.542598Z"},"trusted":true},"execution_count":169,"outputs":[]},{"cell_type":"code","source":"class InvertedResidualBlock(tf.keras.Model):\n    \"\"\"Inverted Residual Block from https://arxiv.org/abs/1801.04381\"\"\"\n    def __init__(self, in_planes, out_planes, expansion_factor, stride=1):\n        super().__init__() # Python 3\n        intermed_planes = in_planes * expansion_factor\n        self.residual = (in_planes == out_planes) and (stride == 1) # Boolean/Condition\n        self.output = nn.Sequential(convbnrelu(stride=1), \n                                    convbnrelu(intermed_planes, stride=stride, groups=intermed_planes), \n                                    convbnrelu(out_planes, stride=stride, act=False))\n    \n    def call(self, x):\n        #residual = x\n        out = self.output(x)\n        if self.residual:\n            return (out + x)#+residual\n        else:\n            return out","metadata":{"id":"gEXj3l0OBi1w","execution":{"iopub.status.busy":"2022-12-17T02:30:41.708479Z","iopub.execute_input":"2022-12-17T02:30:41.708913Z","iopub.status.idle":"2022-12-17T02:30:41.716681Z","shell.execute_reply.started":"2022-12-17T02:30:41.708866Z","shell.execute_reply":"2022-12-17T02:30:41.715757Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"hydranet = HydraNet()\nHydraNet.define_mobilenet = define_mobilenet\nhydranet.define_mobilenet()","metadata":{"id":"lQRVMXFXHIpl","execution":{"iopub.status.busy":"2022-12-17T02:30:42.025403Z","iopub.execute_input":"2022-12-17T02:30:42.025816Z","iopub.status.idle":"2022-12-17T02:30:42.073384Z","shell.execute_reply.started":"2022-12-17T02:30:42.025786Z","shell.execute_reply":"2022-12-17T02:30:42.071693Z"},"trusted":true},"execution_count":172,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/1825423492.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhydranet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHydraNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mHydraNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefine_mobilenet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_mobilenet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhydranet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefine_mobilenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_27/490453672.py\u001b[0m in \u001b[0;36mdefine_mobilenet\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0min_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;31m# number of input channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmobilenet_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mlayer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvbnrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#     c_layer = 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#     for t,c,n,s in (mobilenet_config):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: convbnrelu() missing 2 required positional arguments: 'groups' and 'act'"],"ename":"TypeError","evalue":"convbnrelu() missing 2 required positional arguments: 'groups' and 'act'","output_type":"error"}]},{"cell_type":"code","source":"# def define_mobilenet(self):\n#         mobilenet_config = [[1, 16, 1, 1], # expansion rate, output channels, number of repeats, stride\n#                         [6, 24, 2, 2],\n#                         [6, 32, 3, 2],\n#                         [6, 64, 4, 2],\n#                         [6, 96, 3, 1],\n#                         [6, 160, 3, 2],\n#                         [6, 320, 1, 1],\n#                         ]\n#         self.in_channels = 32 # number of input channels\n#         self.num_layers = len(mobilenet_config)\n#         self.layer1 = convbnrelu(3, self.in_channels, kernel_size=3, stride=2)\n#         c_layer = 2\n#         for t,c,n,s in (mobilenet_config):\n#             layers = []\n#             for idx in range(n):\n#                 layers.append(InvertedResidualBlock(self.in_channels, c, expansion_factor=t, stride=s if idx == 0 else 1))\n#                 self.in_channels = c\n#                 layers._name = f('layer{}'.format(c_layer)) # setattr(object, name, value)\n#             c_layer += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(hydranet)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gh0yhq0d2s8R","outputId":"7d6797ef-bea1-4e59-dbf9-669098250adb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 — Defining the Decoder - A Multi-Task Lighweight RefineNet\nPaper: https://arxiv.org/pdf/1810.03272.pdf\n\n![](https://d3i71xaburhd42.cloudfront.net/4d653b19ce1c7cba79fc2f11271fb90f7744c95c/4-Figure1-1.png)","metadata":{"id":"ymQKYmnjCEng"}},{"cell_type":"code","source":"class CRPBlock(nn.Module):\n    \"\"\"CRP definition\"\"\"\n    def __init__(self, in_planes, out_planes, n_stages, groups=False):\n        super().__init__() #Python 3\n        for i in range(n_stages):\n            setattr(self, '{}_{}'.format(i + 1, 'outvar_dimred'),\n                    conv1x1(in_planes if (i == 0) else out_planes,\n                            out_planes, stride=1,\n                            bias=False, groups=in_planes if groups else 1)) #setattr(object, name, value)\n\n        self.stride = 1\n        self.n_stages = n_stages\n        self.maxpool = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)\n\n    def forward(self, x):\n        top = x\n        for i in range(self.n_stages):\n            top = self.maxpool(top)\n            top = getattr(self, '{}_{}'.format(i + 1, 'outvar_dimred'))(top)#getattr(object, name[, default])\n            x = top + x\n        return x","metadata":{"id":"N6Ia8Cm2BhN6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _make_crp(self, in_planes, out_planes, stages, groups=False):\n    layers = #Call a CRP BLOCK in Layers\n    return nn.Sequential(*layers)\n\nHydraNet._make_crp = _make_crp","metadata":{"id":"mxY4Am_VATCT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def define_lightweight_refinenet(self):\n    ## Light-Weight RefineNet ##\n    self.conv8 = conv1x1(320, 256, bias=False)\n    self.conv7 = conv1x1(160, 256, bias=False)\n    self.conv6 = conv1x1(96, 256, bias=False)\n    self.conv5 = conv1x1(64, 256, bias=False)\n    self.conv4 = conv1x1(32, 256, bias=False)\n    self.conv3 = conv1x1(24, 256, bias=False)\n    self.crp4 = self._make_crp(256, 256, 4, groups=False)\n    self.crp3 = self._make_crp(256, 256, 4, groups=False)\n    self.crp2 = self._make_crp(256, 256, 4, groups=False)\n    self.crp1 = self._make_crp(256, 256, 4, groups=True)\n\n    self.conv_adapt4 = conv1x1(256, 256, bias=False)\n    self.conv_adapt3 = conv1x1(256, 256, bias=False)\n    self.conv_adapt2 = conv1x1(256, 256, bias=False)\n\n    self.pre_depth = #TODO: Define the Purple Pre-Head for Depth\n    self.depth = #TODO: Define the Final layer of Depth\n    self.pre_segm = #TODO: Call the Purple Pre-Head for Segm\n    self.segm = #TODO: Define the Final layer of Segmentation\n    self.relu = #TODO: Define a RELU 6 Operation\n\n    if self.num_tasks == 3:\n        pass\n        #TODO: Create a Normal Head\n\nHydraNet.define_lightweight_refinenet = define_lightweight_refinenet","metadata":{"id":"fdBye5NLBJWv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hydranet.define_lightweight_refinenet()","metadata":{"id":"Q6qYL9W3IPIZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.4 — Define the HydraNet Forward Function\n\n> ![](https://d3i71xaburhd42.cloudfront.net/435d4b5c30f10753d277848a17baddebd98d3c31/2-Figure1-1.png)","metadata":{"id":"XdFV4osfIZNc"}},{"cell_type":"code","source":"def forward(self, x):\n    # MOBILENET V2\n    x = self.layer1(x)\n    x = self.layer2(x) # x / 2\n    l3 = self.layer3(x) # 24, x / 4\n    l4 = self.layer4(l3) # 32, x / 8\n    l5 = self.layer5(l4) # 64, x / 16\n    l6 = self.layer6(l5) # 96, x / 16\n    l7 = self.layer7(l6) # 160, x / 32\n    l8 = self.layer8(l7) # 320, x / 32\n\n    # LIGHT-WEIGHT REFINENET\n    l8 = self.conv8(l8)\n    l7 = self.conv7(l7)\n    l7 = self.relu(l8 + l7)\n    l7 = self.crp4(l7)\n    l7 = self.conv_adapt4(l7)\n    l7 = nn.Upsample(size=l6.size()[2:], mode='bilinear', align_corners=False)(l7)\n\n    l6 = self.conv6(l6)\n    l5 = self.conv5(l5)\n    l5 = self.relu(l5 + l6 + l7)\n    l5 = self.crp3(l5)\n    l5 = self.conv_adapt3(l5)\n    l5 = nn.Upsample(size=l4.size()[2:], mode='bilinear', align_corners=False)(l5)\n\n    l4 = self.conv4(l4)\n    l4 = self.relu(l5 + l4)\n    l4 = self.crp2(l4)\n    l4 = self.conv_adapt2(l4)\n    l4 = nn.Upsample(size=l3.size()[2:], mode='bilinear', align_corners=False)(l4)\n\n    l3 = self.conv3(l3)\n    l3 = self.relu(l3 + l4)\n    l3 = self.crp1(l3)\n\n    # HEADS\n    #TODO: Design the 3 Heads\n    out_segm = \n    out_segm = \n    out_segm = \n\n    out_d = \n    out_d = \n    out_d = \n\n    if self.num_tasks == 3:\n        out_n = \n        out_n = \n        out_n = \n        return out_segm, out_d, out_n\n    else:\n        return out_segm, out_d\n\nHydraNet.forward = forward","metadata":{"id":"32Bwx04u__Cp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 — Run the Model","metadata":{"id":"3KwKZoWT--AD"}},{"cell_type":"markdown","source":"## 3.1 — Load the Model Weights","metadata":{"id":"aXGQHCwtI87U"}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    _ = hydranet.cuda()\n_ = hydranet.eval()","metadata":{"id":"Aty_yRuOTlx_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ckpt = torch.load('ExpKITTI_joint.ckpt')\nhydranet.load_state_dict(ckpt['state_dict'])","metadata":{"id":"Vlidz45grxk3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0b00ad4c-eadb-46ff-bb8b-67cab57baf8d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 — Preprocess Images","metadata":{"id":"DaRUKyEFKUQy"}},{"cell_type":"code","source":"IMG_SCALE  = 1./255\nIMG_MEAN = np.array([0.485, 0.456, 0.406]).reshape((1, 1, 3))\nIMG_STD = np.array([0.229, 0.224, 0.225]).reshape((1, 1, 3))\n\ndef prepare_img(img):\n    return (img * IMG_SCALE - IMG_MEAN) / IMG_STD","metadata":{"id":"7fOiKvtWUQkf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 — Load and Run an Image","metadata":{"id":"ZEnd98Y9KWsZ"}},{"cell_type":"code","source":"# Pre-processing and post-processing constants #\nCMAP = np.load('cmap_kitti.npy')\nNUM_CLASSES = 6","metadata":{"id":"6kWsQzZWTKvW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(CMAP)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZTwp5nMcbtC7","outputId":"43c419bc-6de9-4759-e644-edf1bf340c4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimages_files = glob.glob('data/*.png')\nidx = np.random.randint(0, len(images_files))\n\nimg_path = images_files[idx]\nimg = np.array(Image.open(img_path))\nplt.imshow(img)\nplt.show()","metadata":{"id":"Qk1OkvnvUfyy","colab":{"base_uri":"https://localhost:8080/","height":152},"outputId":"fcc12208-ec51-4163-b004-c1096511a10c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO: Define the Pipeline by filling the Blanks\ndef pipeline(img):\n    with torch.no_grad():\n        img_var = #Put the Image in PYTorch Variable\n        if torch.cuda.is_available():\n            img_var = # Send to GPU\n        segm, depth = # Call the HydraNet\n        segm = #PostProcess / Resize\n        depth = #PostProcess / Resize\n        segm = #Use the CMAP\n        depth = #Take the Absolute Value\n        return depth, segm","metadata":{"id":"k4jSjgJQU1f0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"depth, segm = pipeline(img)","metadata":{"id":"Id8WbXlQLUm8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,20))\nax1.imshow(img)\nax1.set_title('Original', fontsize=30)\nax2.imshow(segm)\nax2.set_title('Predicted Segmentation', fontsize=30)\nax3.imshow(depth, cmap=\"plasma\", vmin=0, vmax=80)\nax3.set_title(\"Predicted Depth\", fontsize=30)\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":182},"id":"A6M_DJ4rPG1m","outputId":"a91523f0-37fc-47a2-f1e9-ce87add230a6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 — Run on a Video","metadata":{"id":"-9309NqKKv0D"}},{"cell_type":"code","source":"print(img.shape)\nprint(depth.shape)\nprint(segm.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Kev1r-xMGwU","outputId":"851741e3-7e1e-4758-8fec-136892cfa3e8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.cm as cm\nimport matplotlib.colors as co\n\ndef depth_to_rgb(depth):\n    normalizer = co.Normalize(vmin=0, vmax=80)\n    mapper = cm.ScalarMappable(norm=normalizer, cmap='plasma')\n    colormapped_im = (mapper.to_rgba(depth)[:, :, :3] * 255).astype(np.uint8)\n    return colormapped_im\n\ndepth_rgb = depth_to_rgb(depth)\nprint(depth_rgb.shape)\nplt.imshow(depth_rgb)\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169},"id":"6OREeqr7WRJF","outputId":"b10d4e10-23fd-4184-842c-f07eae438b88"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(img.shape)\nprint(depth_rgb.shape)\nprint(segm.shape)\nnew_img = np.vstack((img, segm, depth_rgb))\nplt.imshow(new_img)\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":318},"id":"xP2KD52CMnKh","outputId":"5df41906-d2e3-4ba7-febf-d7cb0db11c3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"video_files = sorted(glob.glob(\"data/*.png\"))\n\n# Build a HydraNet\nhydranet = HydraNet()\nhydranet.define_mobilenet()\nhydranet.define_lightweight_refinenet()\nhydranet._initialize_weights()\n\n# Set the Model to Eval on GPU\nif torch.cuda.is_available():\n    _ = hydranet.cuda()\n_ = hydranet.eval()\n\n# Load the Weights\nckpt = torch.load('ExpKITTI_joint.ckpt')\nhydranet.load_state_dict(ckpt['state_dict'])\n\n# Run the pipeline\nresult_video = []\nfor idx, img_path in enumerate(video_files):\n    image = np.array(Image.open(img_path))\n    h, w, _ = image.shape \n    depth, seg = pipeline(image)\n    result_video.append(cv2.cvtColor(cv2.vconcat([image, seg, depth_to_rgb(depth)]), cv2.COLOR_BGR2RGB))\n\nout = cv2.VideoWriter('output/out.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 15, (w,3*h))\n\nfor i in range(len(result_video)):\n    out.write(result_video[i])\nout.release()","metadata":{"id":"l3u7PgoqKuyV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import HTML\nfrom base64 import b64encode\nmp4 = open('output/out.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=800 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)","metadata":{"id":"15-z-lNVxaRX","colab":{"base_uri":"https://localhost:8080/","height":745},"outputId":"23d4c655-c74b-4bbc-9416-736e3caece56"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3D Segmentation\n\nDid you ever wonder... How is segmentation used in self-driving cars? Like, **once you have the map, what do you do with it**?\n<p>\nLet's see something called 3D Segmentation — Fusing a Depth Map with a Segmentation Map!\n<p>\n\nIn my course [MASTER STEREO VISION](https://courses.thinkautonomous.ai/stereo-vision), I teach how to do something called **3D Reconstruction** from a Depth Map and Calibration Parameters. <p>\nIn this course, we're going to see how to do it with Open3D, my go-to library for Point Clouds, and we'll see how to build 3D Segmentation Algorithms by fusing the Depth Map (3D) with the Segmentation Map.","metadata":{"id":"dcSVxVHqp2g6"}},{"cell_type":"code","source":"!pip install open3d==0.14.1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"YGmsHUMgsxHT","outputId":"3d50f1ac-1bed-4911-a9b4-c0e29fc51643"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import open3d as o3d","metadata":{"id":"gl4NqJC7sus2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"o3d.__version__","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"BZUSRdrxwPHx","outputId":"41590081-13c3-4749-cd25-46ee8c1a2bee"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RGBD - Fuse the RGB Image and the Depth Map\n\nThe first thing we'll implement is to create an RGBD Image by fusing the RGB Image with the Depth Map. For that, we'll use [Open3D's Class RGBD Image](http://www.open3d.org/docs/release/python_api/open3d.geometry.RGBDImage.html) and the function create_from_color_and_depth(color, depth).<p>\nIt looks pretty straghtforward, we just need to make sure that the image are loaded as [Open3D Images](http://www.open3d.org/docs/release/python_api/open3d.geometry.Geometry.html?highlight=image#open3d.geometry.Geometry.Image).","metadata":{"id":"a0MwLXd-1Lce"}},{"cell_type":"code","source":"rgbd = #TODO: Call the Function","metadata":{"id":"CduFT43RUaxH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we'll use the function create_from_rgbd_image to build a Point Cloud based on this. For that, we'll need the camera's intrinsic parameters. <p>\nIf you'd like to learn more about this, I invite you to take my course on [Stereo Vision](https://courses.thinkautonomous.ai/stereo-vision). In this course, I'm just going to give'em to you.","metadata":{"id":"osFtnlbzUbp-"}},{"cell_type":"code","source":"o3d.camera.PinholeCameraIntrinsic??","metadata":{"id":"3n9Wuq3fvr5y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"intrinsics = o3d.camera.PinholeCameraIntrinsic(width = 1242, height = 375, fx = 721., fy = 721., cx = 609., cy = 609.)","metadata":{"id":"NZWxv0-Htine"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"point_cloud = #TODO: Create A Point Cloud\no3d.io.write_point_cloud(\"test.pcd\", point_cloud)","metadata":{"id":"m6rpNbLSTon4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e59c16dc-9315-41e9-96c1-e815cc3880d9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3D Segmentation — Fuse the Segmentation Map with the Depth Map\nFrom now on, the process is exactly the same. But instead of creating a Point Cloud from an RGBD Image with the Normal RGB Image, we'll do it with the Depth Map.","metadata":{"id":"dXkvW7hA1Nzl"}},{"cell_type":"code","source":"rgbd = #TODO: Call the Function","metadata":{"id":"8Vt6VWphzsDz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"point_cloud = #TODO: Create A Point Cloud","metadata":{"id":"HDw31-VR0HwC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"o3d.io.write_point_cloud(\"test_segm.pcd\", point_cloud)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IR_UrU-i0wLV","outputId":"0fef3015-2b67-4fd1-d269-402dc1af749e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"UleBi3tUeAST"},"execution_count":null,"outputs":[]}]}