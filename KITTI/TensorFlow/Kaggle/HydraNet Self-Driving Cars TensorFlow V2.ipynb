{"metadata":{"colab":{"name":"MTL_Run_S.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome to the HydraNet Workshop üê∏üê∏üê∏\nIn this workshop, you're going to learn how to build a Neural Network that has:\n* Input: **a monocular RGB Image**\n* Output: **a Depth Map**, and **a Segmentation Map**\n\nA single model, two different outputs. For that, out model will need to use a principle called Multi Task Learning.<p>","metadata":{"id":"20Fk1u9m9xfl"}},{"cell_type":"markdown","source":"# 1 - Imports","metadata":{"id":"5Ix2A28T-ZT_"}},{"cell_type":"code","source":"!pip install -U tensorflow","metadata":{"execution":{"iopub.status.busy":"2022-12-19T20:35:13.384787Z","iopub.execute_input":"2022-12-19T20:35:13.385233Z","iopub.status.idle":"2022-12-19T20:35:26.546119Z","shell.execute_reply.started":"2022-12-19T20:35:13.385194Z","shell.execute_reply":"2022-12-19T20:35:26.544593Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.11.0)\nRequirement already satisfied: tensorboard<2.12,>=2.11 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.11.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (14.0.6)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.15.0)\nRequirement already satisfied: keras<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.11.0)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (22.12.6)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12.1)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.43.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.3.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow) (59.8.0)\nRequirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.11.0)\nRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.19.4)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.7.0)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.21.6)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (4.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.29.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.35.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.3.7)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.1)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->tensorflow) (3.0.9)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.2.4)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.13.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.1.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!wget https://hydranets-data.s3.eu-west-3.amazonaws.com/hydranets-data.zip && unzip -q hydranets-data.zip && mv hydranets-data/* . && rm hydranets-data.zip && rm -rf hydranets-data","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wnk8xflh-RfW","outputId":"651894ce-c17f-4b40-8b22-fdbfca19c527","execution":{"iopub.status.busy":"2022-12-19T20:36:01.180969Z","iopub.execute_input":"2022-12-19T20:36:01.181377Z","iopub.status.idle":"2022-12-19T20:36:11.283624Z","shell.execute_reply.started":"2022-12-19T20:36:01.181347Z","shell.execute_reply":"2022-12-19T20:36:11.282358Z"},"trusted":true},"execution_count":140,"outputs":[{"name":"stdout","text":"--2022-12-19 20:36:02--  https://hydranets-data.s3.eu-west-3.amazonaws.com/hydranets-data.zip\nResolving hydranets-data.s3.eu-west-3.amazonaws.com (hydranets-data.s3.eu-west-3.amazonaws.com)... 3.5.226.127\nConnecting to hydranets-data.s3.eu-west-3.amazonaws.com (hydranets-data.s3.eu-west-3.amazonaws.com)|3.5.226.127|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 110752264 (106M) [application/zip]\nSaving to: ‚Äòhydranets-data.zip‚Äô\n\nhydranets-data.zip  100%[===================>] 105.62M  16.9MB/s    in 7.1s    \n\n2022-12-19 20:36:09 (15.0 MB/s) - ‚Äòhydranets-data.zip‚Äô saved [110752264/110752264]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# import os\n# import shutil\n# shutil.rmtree('/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2022-12-19T20:36:18.525199Z","iopub.execute_input":"2022-12-19T20:36:18.525728Z","iopub.status.idle":"2022-12-19T20:36:18.531166Z","shell.execute_reply.started":"2022-12-19T20:36:18.525677Z","shell.execute_reply":"2022-12-19T20:36:18.529968Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nimport math","metadata":{"id":"szGHb1YQSyJ_","execution":{"iopub.status.busy":"2022-12-19T20:36:30.404759Z","iopub.execute_input":"2022-12-19T20:36:30.405764Z","iopub.status.idle":"2022-12-19T20:36:30.412164Z","shell.execute_reply.started":"2022-12-19T20:36:30.405726Z","shell.execute_reply":"2022-12-19T20:36:30.411080Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"markdown","source":"# 2 ‚Äî Creating the HydraNet\nWe now have 2 DataLoaders: one for training, and one for validation/test. <p>\n\nIn the next step, we're going to define our model, following the paper [Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations](https://arxiv.org/pdf/1809.04766.pdf) ‚Äî‚Äî If you haven't read it yet, now is the time.<p>\n\nA Note ‚Äî This notebook has been adapted from DrSleep, a researcher named Vladimir, who authorized me to adapt it for education purposes. [Here's the notebook I'm refering to](https://github.com/DrSleep/multi-task-refinenet/blob/master/src/notebooks/ExpNYUDKITTI_joint.ipynb/).\n\n<p>\n\n> ![](https://d3i71xaburhd42.cloudfront.net/435d4b5c30f10753d277848a17baddebd98d3c31/2-Figure1-1.png)\n\nOur model takes an input RGB image, make it go through an encoder, a lightweight refinenet decoder, and then has 2 heads, one for each task.<p>\nThings to note:\n* The only **convolutions** we'll need will be 3x3 and 1x1\n* We also need a **MaxPooling 5x5**\n* **CRP-Blocks** are implemented as Skip-Connection Operations\n* **Each Head is made of a 1x1 convolution followed by a 3x3 convolution**, only the data and the loss change there\n","metadata":{"id":"LuR-bk-13cX6"}},{"cell_type":"markdown","source":"## 2.1 ‚Äî Create a HydraNet class","metadata":{"id":"Pf8671mIE_tU"}},{"cell_type":"code","source":"class HydraNet(tf.keras.Model):\n    def __init__(self):        \n        super(HydraNet, self).__init__() # Python 3\n        self.num_tasks = 2\n        self.num_classes = 6\n        \n#     def Encoder(self):\n#         mobilenet_config =[[1, 16, 1, 1],\n#                     [6, 24, 2, 2],\n#                     [6, 32, 3, 2],\n#                     [6, 64, 4, 2],\n#                     [6, 96, 3, 1],\n#                     [6, 160, 3, 2],\n#                     [6, 320, 1, 1],\n#                     ]\n#         c_layer = 2\n#         in_channels = 32\n#         layer1 = tf.keras.Sequential(convbnrelu(filters=32, kernel_size=3, stride=2))\n#         layer1._name = 'layer1'\n#         encoder = tf.keras.Sequential()\n#         encoder.add(layer1)\n#         for t,c,n,s in (mobilenet_config):\n#             for i in range(n) :\n#                 encoder.add(InvertedResidualBlock(in_channels, c, t, stride=s if i==0 else 1, name=f'layer{c_layer}'))\n#                 in_channels = c\n#                 c_layer += 1\n\n#         return encoder\n","metadata":{"execution":{"iopub.status.busy":"2022-12-19T20:36:31.524061Z","iopub.execute_input":"2022-12-19T20:36:31.524855Z","iopub.status.idle":"2022-12-19T20:36:31.530758Z","shell.execute_reply.started":"2022-12-19T20:36:31.524812Z","shell.execute_reply":"2022-12-19T20:36:31.529870Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"net = HydraNet()","metadata":{"execution":{"iopub.status.busy":"2022-12-19T20:36:32.786110Z","iopub.execute_input":"2022-12-19T20:36:32.787144Z","iopub.status.idle":"2022-12-19T20:36:32.797599Z","shell.execute_reply.started":"2022-12-19T20:36:32.787105Z","shell.execute_reply":"2022-12-19T20:36:32.796098Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"markdown","source":"```\nLayer(1) S1\n    conv2d(32, k=3, s=2, padding=1, bias=False)\n    batchnorm(eps=1e-05, momentum=0.1)\n    relu(6) \n    \nLayer(2) IRB\n    conv2d(32, k=1, s=1, bias=False)\n    batchnorm(eps=1e-05, momentum=0.1)\n    relu(6) \n    \n    \n```    ","metadata":{"execution":{"iopub.status.busy":"2022-12-17T01:53:05.397541Z","iopub.execute_input":"2022-12-17T01:53:05.398003Z","iopub.status.idle":"2022-12-17T01:53:05.405818Z","shell.execute_reply.started":"2022-12-17T01:53:05.397970Z","shell.execute_reply":"2022-12-17T01:53:05.404133Z"}}},{"cell_type":"markdown","source":"## 2.2 ‚Äî¬†Defining the Encoder: A MobileNetv2\n![](https://iq.opengenus.org/content/images/2020/11/conv_mobilenet_v2.jpg)","metadata":{"id":"025K4utrBqNy"}},{"cell_type":"code","source":"def conv3x3(filters, stride=1, bias=False, dilation=1, groups=1):\n    # 3x3 convolution\n    return tf.keras.layers.Conv2D(filters, kernel_size=3, strides=stride,\n                     padding='same', dilation_rate=dilation, use_bias=bias, groups=groups)","metadata":{"id":"TpCvdaopBGqo","execution":{"iopub.status.busy":"2022-12-19T20:36:35.123694Z","iopub.execute_input":"2022-12-19T20:36:35.124134Z","iopub.status.idle":"2022-12-19T20:36:35.131180Z","shell.execute_reply.started":"2022-12-19T20:36:35.124102Z","shell.execute_reply":"2022-12-19T20:36:35.129858Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"code","source":"# Test conv3x3\nconv3x3(filters=32)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T20:36:35.780121Z","iopub.execute_input":"2022-12-19T20:36:35.780559Z","iopub.status.idle":"2022-12-19T20:36:35.791338Z","shell.execute_reply.started":"2022-12-19T20:36:35.780502Z","shell.execute_reply":"2022-12-19T20:36:35.789626Z"},"trusted":true},"execution_count":147,"outputs":[{"execution_count":147,"output_type":"execute_result","data":{"text/plain":"<keras.layers.convolutional.conv2d.Conv2D at 0x7f8e9c74a510>"},"metadata":{}}]},{"cell_type":"code","source":"def conv1x1(filters, stride=1, bias=False, groups=1):\n    # 1x1 convolution\n    return tf.keras.layers.Conv2D(filters, kernel_size=1, strides=stride,\n                     padding='valid', use_bias=bias, groups=groups)","metadata":{"id":"X26XhiFLBZD5","execution":{"iopub.status.busy":"2022-12-19T20:36:36.519105Z","iopub.execute_input":"2022-12-19T20:36:36.519586Z","iopub.status.idle":"2022-12-19T20:36:36.526116Z","shell.execute_reply.started":"2022-12-19T20:36:36.519549Z","shell.execute_reply":"2022-12-19T20:36:36.524611Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"# Test conv1x1\nconv1x1(filters=32)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T20:36:37.554974Z","iopub.execute_input":"2022-12-19T20:36:37.555383Z","iopub.status.idle":"2022-12-19T20:36:37.566423Z","shell.execute_reply.started":"2022-12-19T20:36:37.555352Z","shell.execute_reply":"2022-12-19T20:36:37.564920Z"},"trusted":true},"execution_count":149,"outputs":[{"execution_count":149,"output_type":"execute_result","data":{"text/plain":"<keras.layers.convolutional.conv2d.Conv2D at 0x7f8e9c74fb50>"},"metadata":{}}]},{"cell_type":"code","source":"def batchnorm():\n    # batch norm 2d\n    batch_norm = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1)\n    batch_norm.trainable = True\n    return batch_norm","metadata":{"execution":{"iopub.status.busy":"2022-12-19T20:36:37.935885Z","iopub.execute_input":"2022-12-19T20:36:37.936327Z","iopub.status.idle":"2022-12-19T20:36:37.942275Z","shell.execute_reply.started":"2022-12-19T20:36:37.936294Z","shell.execute_reply":"2022-12-19T20:36:37.941039Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"# Test batchnorm\nbatchnorm()","metadata":{"execution":{"iopub.status.busy":"2022-12-19T20:36:38.389345Z","iopub.execute_input":"2022-12-19T20:36:38.389938Z","iopub.status.idle":"2022-12-19T20:36:38.401439Z","shell.execute_reply.started":"2022-12-19T20:36:38.389883Z","shell.execute_reply":"2022-12-19T20:36:38.400058Z"},"trusted":true},"execution_count":151,"outputs":[{"execution_count":151,"output_type":"execute_result","data":{"text/plain":"<keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f8e9c753bd0>"},"metadata":{}}]},{"cell_type":"code","source":"def convbnrelu(filters, kernel_size, stride=1, groups=1, act=True):\n    # conv-batchnorm-relu\n    if int(kernel_size/2) == 1 :\n        padding = 'same'\n    if int(kernel_size/2) == 0 :\n        padding = 'valid'\n    if act:\n        return tf.keras.Sequential([tf.keras.layers.Conv2D(filters, kernel_size, strides=stride, padding=padding, groups=groups, use_bias=False),\n                             batchnorm(),\n                             tf.keras.layers.ReLU(max_value=6)])\n    else:\n        return tf.keras.Sequential([tf.keras.layers.Conv2D(filters, kernel_size, strides=stride, padding=padding, groups=groups, use_bias=False),\n                             batchnorm()])","metadata":{"id":"ytAJff2_QJym","execution":{"iopub.status.busy":"2022-12-19T20:36:39.063401Z","iopub.execute_input":"2022-12-19T20:36:39.063852Z","iopub.status.idle":"2022-12-19T20:36:39.074535Z","shell.execute_reply.started":"2022-12-19T20:36:39.063817Z","shell.execute_reply":"2022-12-19T20:36:39.072996Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"code","source":"# Test convbnrelu\ndisplay(convbnrelu(32,3,1,1,True).layers)\nprint()\ndisplay(convbnrelu(32,3,1,1,False).layers)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T20:36:40.120223Z","iopub.execute_input":"2022-12-19T20:36:40.120660Z","iopub.status.idle":"2022-12-19T20:36:40.148188Z","shell.execute_reply.started":"2022-12-19T20:36:40.120624Z","shell.execute_reply":"2022-12-19T20:36:40.147332Z"},"trusted":true},"execution_count":153,"outputs":[{"output_type":"display_data","data":{"text/plain":"[<keras.layers.convolutional.conv2d.Conv2D at 0x7f8e9c753950>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f8e9c74f890>,\n <keras.layers.activation.relu.ReLU at 0x7f8f769b0d50>]"},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[<keras.layers.convolutional.conv2d.Conv2D at 0x7f8f769b0d50>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f8e9c74a4d0>]"},"metadata":{}}]},{"cell_type":"code","source":"class InvertedResidualBlock(tf.keras.Model) :\n    def __init__(self,in_planes, filters, expansion_factor, stride) :\n        super(InvertedResidualBlock, self).__init__()\n        intermed_planes = in_planes * expansion_factor\n        self.residual = (in_planes == filters) and (stride == 1) # Boolean/Condition\n        self.IBR = tf.keras.Sequential([convbnrelu(in_planes, kernel_size=1, stride=stride, act=True), \n                               convbnrelu(intermed_planes, kernel_size=3, \n                                          stride=stride, groups=intermed_planes, act=True), \n                               convbnrelu(filters, kernel_size=1, stride=stride, act=False)])\n        \n        def call(self, inputs) :\n            x = self.IBR(inputs)\n            if self.residual :\n                return (x + inputs)\n            else :\n                return x","metadata":{"execution":{"iopub.status.busy":"2022-12-19T20:36:40.345152Z","iopub.execute_input":"2022-12-19T20:36:40.345614Z","iopub.status.idle":"2022-12-19T20:36:40.354900Z","shell.execute_reply.started":"2022-12-19T20:36:40.345575Z","shell.execute_reply":"2022-12-19T20:36:40.353572Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"code","source":" def define_mobilenet(self):\n        layers = []\n        mobilenet_config =[[1, 16, 1, 1],\n                    [6, 24, 2, 2],\n                    [6, 32, 3, 2],\n                    [6, 64, 4, 2],\n                    [6, 96, 3, 1],\n                    [6, 160, 3, 2],\n                    [6, 320, 1, 1],\n                    ]\n        in_channels = 32\n        c_layer = 2\n        layer1 = tf.keras.Sequential(convbnrelu(filters=32, kernel_size=3, stride=2))\n        layer1._name = 'layer1'\n        layers.append(layer1)\n        encoder = tf.keras.Sequential()\n        encoder.add(layer1)\n        layer_num = 2\n        for t,c,n,s in (mobilenet_config):\n            ibr = tf.keras.Sequential()\n            for i in range(n) :\n                block = InvertedResidualBlock(in_channels, c, t, stride=s if i==0 else 1)\n                in_channels = c\n                c_layer += 1\n            ibr.add(block)\n            ibr._name = f'layer{layer_num}'\n            layer_num += 1\n            layers.append(ibr)\n\n        return encoder, layers","metadata":{"execution":{"iopub.status.busy":"2022-12-19T20:36:40.574049Z","iopub.execute_input":"2022-12-19T20:36:40.574541Z","iopub.status.idle":"2022-12-19T20:36:40.585619Z","shell.execute_reply.started":"2022-12-19T20:36:40.574485Z","shell.execute_reply":"2022-12-19T20:36:40.584239Z"},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"code","source":"# def define_mobilenet(self):\n#     mobilenet_config = [[1, 16, 1, 1], # expansion rate, output channels, number of repeats, stride\n#                     [6, 24, 2, 2],\n#                     [6, 32, 3, 2],\n#                     [6, 64, 4, 2],\n#                     [6, 96, 3, 1],\n#                     [6, 160, 3, 2],\n#                     [6, 320, 1, 1],\n#                     ]\n#     self.in_channels = 32 # number of input channels\n#     self.num_layers = len(mobilenet_config)\n#     self.layer1 = convbnrelu(3, self.in_channels, kernel_size=3, stride=2) # This is the first layer of the first \n#     c_layer = 2\n#     for t,c,n,s in (mobilenet_config):\n#         layers = []\n#         for idx in range(n):\n#             layers.append(InvertedResidualBlock(self.in_channels, c, expansion_factor=t, stride=s if idx == 0 else 1))\n#             self.in_channels = c\n#         setattr(self, 'layer{}'.format(c_layer), nn.Sequential(*layers)) # setattr(object, name, value)\n#         c_layer += 1\n\n# HydraNet.define_mobilenet = define_mobilenet","metadata":{"execution":{"iopub.status.busy":"2022-12-19T20:36:40.774436Z","iopub.execute_input":"2022-12-19T20:36:40.775028Z","iopub.status.idle":"2022-12-19T20:36:40.780274Z","shell.execute_reply.started":"2022-12-19T20:36:40.774989Z","shell.execute_reply":"2022-12-19T20:36:40.779314Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"code","source":"# net.define_mobilenet()\n# for model in net.define_mobilenet()[1] :\n#     print(model._name)   ","metadata":{"execution":{"iopub.status.busy":"2022-12-19T20:36:41.021492Z","iopub.execute_input":"2022-12-19T20:36:41.021992Z","iopub.status.idle":"2022-12-19T20:36:41.026359Z","shell.execute_reply.started":"2022-12-19T20:36:41.021952Z","shell.execute_reply":"2022-12-19T20:36:41.025109Z"},"trusted":true},"execution_count":157,"outputs":[]},{"cell_type":"code","source":"def define_mobilenet(self):\n    LAYERS=[]\n    mobilenet_config = [[1, 16, 1, 1], # expansion rate, output channels, number of repeats, stride\n                    [6, 24, 2, 2],\n                    [6, 32, 3, 2],\n                    [6, 64, 4, 2],\n                    [6, 96, 3, 1],\n                    [6, 160, 3, 2],\n                    [6, 320, 1, 1],\n                    ]\n    self.in_channels = 32 # number of input channels\n    self.num_layers = len(mobilenet_config)\n    self.layer1 = convbnrelu(filters=32, kernel_size=3, stride=2) # This is the first layer of the first \n    layer1_model = tf.keras.Sequential(self.layer1)\n    layer1_model._name = 'layer1'\n    LAYERS.append(layer1_model)\n    encoder = tf.keras.Sequential()\n    encoder.add(layer1_model)\n    c_layer = 2\n    for t,c,n,s in (mobilenet_config):\n        layers = []\n        \n        for idx in range(n):\n            layers.append(InvertedResidualBlock(self.in_channels, c, expansion_factor=t, stride=s if idx == 0 else 1))\n            self.in_channels = c\n        model = tf.keras.Sequential(layers)\n        model._name = f'layer{c_layer}'\n        # print(model._name)\n        encoder.add(model)\n        LAYERS.append(tf.keras.Sequential(layers))\n        c_layer += 1\n        \n    for model, i in zip(LAYERS[1:], range(2,9)):\n        model._name = f'layer{i}'\n        \n    return encoder, LAYERS\n\n# HydraNet.define_mobilenet = define_mobilenet","metadata":{"execution":{"iopub.status.busy":"2022-12-19T20:43:43.263301Z","iopub.execute_input":"2022-12-19T20:43:43.263780Z","iopub.status.idle":"2022-12-19T20:43:43.278449Z","shell.execute_reply.started":"2022-12-19T20:43:43.263735Z","shell.execute_reply":"2022-12-19T20:43:43.277602Z"},"trusted":true},"execution_count":205,"outputs":[]},{"cell_type":"code","source":"HydraNet.define_mobilenet = define_mobilenet","metadata":{"execution":{"iopub.status.busy":"2022-12-19T20:43:43.710382Z","iopub.execute_input":"2022-12-19T20:43:43.710883Z","iopub.status.idle":"2022-12-19T20:43:43.716205Z","shell.execute_reply.started":"2022-12-19T20:43:43.710843Z","shell.execute_reply":"2022-12-19T20:43:43.714905Z"},"trusted":true},"execution_count":206,"outputs":[]},{"cell_type":"markdown","source":"```\nlayer1 = 1,1\nlayer2 = 1,1\nlayer3 = 2,2\nlayer4 = 3,3\nlayer5 = 4,4\nlayer6 = 3,3\nlayer7 = 3,3\nlayer8 = 1,1\n```","metadata":{}},{"cell_type":"code","source":"for model in net.define_mobilenet()[1] :\n    print(model._name)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T20:45:34.938119Z","iopub.execute_input":"2022-12-19T20:45:34.938585Z","iopub.status.idle":"2022-12-19T20:45:35.375541Z","shell.execute_reply.started":"2022-12-19T20:45:34.938546Z","shell.execute_reply":"2022-12-19T20:45:35.373782Z"},"trusted":true},"execution_count":215,"outputs":[{"name":"stdout","text":"layer1\nlayer2\nlayer3\nlayer4\nlayer5\nlayer6\nlayer7\nlayer8\n","output_type":"stream"}]},{"cell_type":"code","source":"# layers = [tf.keras.layers.Dense(10, input_shape=(5,)), tf.keras.layers.Dense(5), tf.keras.layers.Dense(1)]\n\n# tf.keras.Sequentialacosh(layers).summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-19T02:33:40.327828Z","iopub.execute_input":"2022-12-19T02:33:40.328309Z","iopub.status.idle":"2022-12-19T02:33:40.334949Z","shell.execute_reply.started":"2022-12-19T02:33:40.328274Z","shell.execute_reply":"2022-12-19T02:33:40.333155Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"<br>idx : 0\nt : 1 \tc : 16 \tn : 1 \ts : 1 \tin_channels : 16</br>\n\n<br>idx : 0\nt : 6 \tc : 24 \tn : 2 \ts : 2 \tin_channels : 24</br>\nidx : 1\nt : 6 \tc : 24 \tn : 2 \ts : 2 \tin_channels : 24</br>\n\n\n<br>idx : 0\nt : 6 \tc : 32 \tn : 3 \ts : 2 \tin_channels : 32</br>\nidx : 1\nt : 6 \tc : 32 \tn : 3 \ts : 2 \tin_channels : 32</br>\nidx : 2\nt : 6 \tc : 32 \tn : 3 \ts : 2 \tin_channels : 32</br>\n\n\n<br>idx : 0\nt : 6 \tc : 64 \tn : 4 \ts : 2 \tin_channels : 64</br>\nidx : 1\nt : 6 \tc : 64 \tn : 4 \ts : 2 \tin_channels : 64</br>\nidx : 2\nt : 6 \tc : 64 \tn : 4 \ts : 2 \tin_channels : 64</br>\nidx : 3\nt : 6 \tc : 64 \tn : 4 \ts : 2 \tin_channels : 64</br>\n\n\n<br>idx : 0\nt : 6 \tc : 96 \tn : 3 \ts : 1 \tin_channels : 96</br>\nidx : 1\nt : 6 \tc : 96 \tn : 3 \ts : 1 \tin_channels : 96</br>\nidx : 2\nt : 6 \tc : 96 \tn : 3 \ts : 1 \tin_channels : 96</br>\n\n\n<br>idx : 0\nt : 6 \tc : 160 \tn : 3 \ts : 2 \tin_channels : 160</br>\nidx : 1\nt : 6 \tc : 160 \tn : 3 \ts : 2 \tin_channels : 160</br>\nidx : 2\nt : 6 \tc : 160 \tn : 3 \ts : 2 \tin_channels : 160</br>\n\n\n<br>idx : 0\nt : 6 \tc : 320 \tn : 1 \ts : 1 \tin_channels : 320</br>","metadata":{}},{"cell_type":"code","source":"# mobilenet_config =[[1, 16, 1, 1],\n#                 [6, 24, 2, 2],\n#                 [6, 32, 3, 2],\n#                 [6, 64, 4, 2],\n#                 [6, 96, 3, 1],\n#                 [6, 160, 3, 2],\n#                 [6, 320, 1, 1],\n#                 ]\n# in_channels = 32 # number of input channels\n# num_layers = len(mobilenet_config)\n# layer1 = convbnrelu(filters=3, kernel_size=3, stride=2, groups=1, act=True)\n# c_layer = 2\n# for t,c,n,s in (mobilenet_config):\n# #         layers = []\n#     for idx in range(n):\n#         print('idx :', idx)\n#         #layers.append(InvertedResidualBlock(in_channels, c, expansion_factor=t, stride=s if idx == 0 else 1))\n#         in_channels = c\n#         print('t :', t, '\\tc :', c, '\\tn :', n, '\\ts :', s, '\\tin_channels :', in_channels)\n#     print('************************************************')\n#     c_layer += 1","metadata":{"execution":{"iopub.status.busy":"2022-12-19T04:32:12.563527Z","iopub.execute_input":"2022-12-19T04:32:12.564013Z","iopub.status.idle":"2022-12-19T04:32:12.588453Z","shell.execute_reply.started":"2022-12-19T04:32:12.563896Z","shell.execute_reply":"2022-12-19T04:32:12.587495Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# def MobileNetV2(input_image = (None,None,3), n_classes=6):\n#     inputs = Input (input_shape)\n#     x = Conv2D(32,3,strides=(2,2),padding='same', use_bias=False)(input)\n#     x = BatchNormalization(name='conv1_bn')(x)\n#     x = ReLU(6, name='conv1_relu')(x)\n#     # 17 Bottlenecks\n#     x = depthwise_block(x,stride=1,block_id=1)\n#     x = projection_block(x, out_channels=16,block_id=1)\n#     x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 24, stride = 2,block_id = 2)\n#     x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 24, stride = 1,block_id = 3)\n#     x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 32, stride = 2,block_id = 4)\n#     x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 32, stride = 1,block_id = 5)\n#     x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 32, stride = 1,block_id = 6)\n#     x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 2,block_id = 7)\n#     x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 1,block_id = 8)\n#     x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 1,block_id = 9)\n#     x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 1,block_id = 10)\n#     x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 96, stride = 1,block_id = 11)\n#     x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 96, stride = 1,block_id = 12)\n#     x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 96, stride = 1,block_id = 13)\n#     x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 160, stride = 2,block_id = 14)\n#     x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 160, stride = 1,block_id = 15)\n#     x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 160, stride = 1,block_id = 16)\n#     x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 320, stride = 1,block_id = 17)\n#     x = Conv2D(filters = 1280,kernel_size = 1,padding='same',use_bias=False, name = 'last_conv')(x)\n#     x = BatchNormalization(name='last_bn')(x)\n#     x = ReLU(6,name='last_relu')(x)\n#     x = GlobalAveragePooling2D(name='global_average_pool')(x)\n#     output = Dense(n_classes,activation='softmax')(x)\n#     model = Model(inputs, output)\n#     return model","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gh0yhq0d2s8R","outputId":"7d6797ef-bea1-4e59-dbf9-669098250adb","execution":{"iopub.status.busy":"2022-12-17T03:22:16.252010Z","iopub.execute_input":"2022-12-17T03:22:16.252398Z","iopub.status.idle":"2022-12-17T03:22:16.270410Z","shell.execute_reply.started":"2022-12-17T03:22:16.252366Z","shell.execute_reply":"2022-12-17T03:22:16.269157Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"MobileNetV2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 ‚Äî Defining the Decoder - A Multi-Task Lighweight RefineNet\nPaper: https://arxiv.org/pdf/1810.03272.pdf\n\n![](https://d3i71xaburhd42.cloudfront.net/4d653b19ce1c7cba79fc2f11271fb90f7744c95c/4-Figure1-1.png)","metadata":{"id":"ymQKYmnjCEng"}},{"cell_type":"code","source":"class CRPBlock(nn.Module):\n    \"\"\"CRP definition\"\"\"\n    def __init__(self, in_planes, out_planes, n_stages, groups=False):\n        super().__init__() #Python 3\n        for i in range(n_stages):\n            setattr(self, '{}_{}'.format(i + 1, 'outvar_dimred'),\n                    conv1x1(in_planes if (i == 0) else out_planes,\n                            out_planes, stride=1,\n                            bias=False, groups=in_planes if groups else 1)) #setattr(object, name, value)\n\n        self.stride = 1\n        self.n_stages = n_stages\n        self.maxpool = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)\n\n    def forward(self, x):\n        top = x\n        for i in range(self.n_stages):\n            top = self.maxpool(top)\n            top = getattr(self, '{}_{}'.format(i + 1, 'outvar_dimred'))(top)#getattr(object, name[, default])\n            x = top + x\n        return x","metadata":{"id":"N6Ia8Cm2BhN6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _make_crp(self, in_planes, out_planes, stages, groups=False):\n    layers = #Call a CRP BLOCK in Layers\n    return nn.Sequential(*layers)\n\nHydraNet._make_crp = _make_crp","metadata":{"id":"mxY4Am_VATCT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def define_lightweight_refinenet(self):\n    ## Light-Weight RefineNet ##\n    self.conv8 = conv1x1(320, 256, bias=False)\n    self.conv7 = conv1x1(160, 256, bias=False)\n    self.conv6 = conv1x1(96, 256, bias=False)\n    self.conv5 = conv1x1(64, 256, bias=False)\n    self.conv4 = conv1x1(32, 256, bias=False)\n    self.conv3 = conv1x1(24, 256, bias=False)\n    self.crp4 = self._make_crp(256, 256, 4, groups=False)\n    self.crp3 = self._make_crp(256, 256, 4, groups=False)\n    self.crp2 = self._make_crp(256, 256, 4, groups=False)\n    self.crp1 = self._make_crp(256, 256, 4, groups=True)\n\n    self.conv_adapt4 = conv1x1(256, 256, bias=False)\n    self.conv_adapt3 = conv1x1(256, 256, bias=False)\n    self.conv_adapt2 = conv1x1(256, 256, bias=False)\n\n    self.pre_depth = conv1x1(256, #TODO: Define the Purple Pre-Head for Depth\n    self.depth = #TODO: Define the Final layer of Depth\n    self.pre_segm = #TODO: Call the Purple Pre-Head for Segm\n    self.segm = #TODO: Define the Final layer of Segmentation\n    self.relu = #TODO: Define a RELU 6 Operation\n\n    if self.num_tasks == 3:\n        pass\n        #TODO: Create a Normal Head\n\nHydraNet.define_lightweight_refinenet = define_lightweight_refinenet","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hydranet.define_lightweight_refinenet()","metadata":{"id":"Q6qYL9W3IPIZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.4 ‚Äî Define the HydraNet Forward Function\n\n> ![](https://d3i71xaburhd42.cloudfront.net/435d4b5c30f10753d277848a17baddebd98d3c31/2-Figure1-1.png)","metadata":{"id":"XdFV4osfIZNc"}},{"cell_type":"code","source":"def forward(self, x):\n    # MOBILENET V2\n    x = self.layer1(x)\n    x = self.layer2(x) # x / 2\n    l3 = self.layer3(x) # 24, x / 4\n    l4 = self.layer4(l3) # 32, x / 8\n    l5 = self.layer5(l4) # 64, x / 16\n    l6 = self.layer6(l5) # 96, x / 16\n    l7 = self.layer7(l6) # 160, x / 32\n    l8 = self.layer8(l7) # 320, x / 32\n\n    # LIGHT-WEIGHT REFINENET\n    l8 = self.conv8(l8)\n    l7 = self.conv7(l7)\n    l7 = self.relu(l8 + l7)\n    l7 = self.crp4(l7)\n    l7 = self.conv_adapt4(l7)\n    l7 = nn.Upsample(size=l6.size()[2:], mode='bilinear', align_corners=False)(l7)\n\n    l6 = self.conv6(l6)\n    l5 = self.conv5(l5)\n    l5 = self.relu(l5 + l6 + l7)\n    l5 = self.crp3(l5)\n    l5 = self.conv_adapt3(l5)\n    l5 = nn.Upsample(size=l4.size()[2:], mode='bilinear', align_corners=False)(l5)\n\n    l4 = self.conv4(l4)\n    l4 = self.relu(l5 + l4)\n    l4 = self.crp2(l4)\n    l4 = self.conv_adapt2(l4)\n    l4 = nn.Upsample(size=l3.size()[2:], mode='bilinear', align_corners=False)(l4)\n\n    l3 = self.conv3(l3)\n    l3 = self.relu(l3 + l4)\n    l3 = self.crp1(l3)\n\n    # HEADS\n    #TODO: Design the 3 Heads\n    out_segm = \n    out_segm = \n    out_segm = \n\n    out_d = \n    out_d = \n    out_d = \n\n    if self.num_tasks == 3:\n        out_n = \n        out_n = \n        out_n = \n        return out_segm, out_d, out_n\n    else:\n        return out_segm, out_d\n\nHydraNet.forward = forward","metadata":{"id":"32Bwx04u__Cp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 ‚Äî¬†Run the Model","metadata":{"id":"3KwKZoWT--AD"}},{"cell_type":"markdown","source":"## 3.1 ‚Äî¬†Load the Model Weights","metadata":{"id":"aXGQHCwtI87U"}},{"cell_type":"code","source":"# if torch.cuda.is_available():\n#     _ = hydranet.cuda()\n# _ = hydranet.eval()","metadata":{"id":"Aty_yRuOTlx_","execution":{"iopub.status.busy":"2022-12-19T22:20:17.654183Z","iopub.execute_input":"2022-12-19T22:20:17.654877Z","iopub.status.idle":"2022-12-19T22:20:17.661446Z","shell.execute_reply.started":"2022-12-19T22:20:17.654807Z","shell.execute_reply":"2022-12-19T22:20:17.659774Z"},"trusted":true},"execution_count":217,"outputs":[]},{"cell_type":"code","source":"ckpt = tf.train.load_checkpoint('ExpKITTI_joint.ckpt') #torch.load('ExpKITTI_joint.ckpt')\nhydranet.load_state_dict(ckpt['state_dict'])","metadata":{"id":"Vlidz45grxk3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0b00ad4c-eadb-46ff-bb8b-67cab57baf8d","execution":{"iopub.status.busy":"2022-12-19T22:20:47.507439Z","iopub.execute_input":"2022-12-19T22:20:47.508259Z","iopub.status.idle":"2022-12-19T22:20:47.557999Z","shell.execute_reply.started":"2022-12-19T22:20:47.508210Z","shell.execute_reply":"2022-12-19T22:20:47.556131Z"},"trusted":true},"execution_count":218,"outputs":[{"name":"stderr","text":"2022-12-19 22:20:47.516330: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open ExpKITTI_joint.ckpt: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m   \u001b[0;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Unable to open table file ExpKITTI_joint.ckpt: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/259846590.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ExpKITTI_joint.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#torch.load('ExpKITTI_joint.ckpt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhydranet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/checkpoint_utils.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(ckpt_dir_or_file)\u001b[0m\n\u001b[1;32m     66\u001b[0m     raise ValueError(\"Couldn't find 'checkpoint' file or checkpoints in \"\n\u001b[1;32m     67\u001b[0m                      \"given directory %s\" % ckpt_dir_or_file)\n\u001b[0;32m---> 68\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0merror_translator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;34m'Unable to open table file'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m   elif 'Failed to find the saved tensor slices' in error_message or (\n\u001b[1;32m     42\u001b[0m       'not convertible to numpy dtype' in error_message):\n","\u001b[0;31mDataLossError\u001b[0m: Unable to open table file ExpKITTI_joint.ckpt: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?"],"ename":"DataLossError","evalue":"Unable to open table file ExpKITTI_joint.ckpt: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?","output_type":"error"}]},{"cell_type":"markdown","source":"## 3.2 ‚Äî Preprocess Images","metadata":{"id":"DaRUKyEFKUQy"}},{"cell_type":"code","source":"IMG_SCALE  = 1./255\nIMG_MEAN = np.array([0.485, 0.456, 0.406]).reshape((1, 1, 3))\nIMG_STD = np.array([0.229, 0.224, 0.225]).reshape((1, 1, 3))\n\ndef prepare_img(img):\n    return (img * IMG_SCALE - IMG_MEAN) / IMG_STD","metadata":{"id":"7fOiKvtWUQkf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 ‚Äî Load and Run an Image","metadata":{"id":"ZEnd98Y9KWsZ"}},{"cell_type":"code","source":"# Pre-processing and post-processing constants #\nCMAP = np.load('cmap_kitti.npy')\nNUM_CLASSES = 6","metadata":{"id":"6kWsQzZWTKvW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(CMAP)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZTwp5nMcbtC7","outputId":"43c419bc-6de9-4759-e644-edf1bf340c4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimages_files = glob.glob('data/*.png')\nidx = np.random.randint(0, len(images_files))\n\nimg_path = images_files[idx]\nimg = np.array(Image.open(img_path))\nplt.imshow(img)\nplt.show()","metadata":{"id":"Qk1OkvnvUfyy","colab":{"base_uri":"https://localhost:8080/","height":152},"outputId":"fcc12208-ec51-4163-b004-c1096511a10c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO: Define the Pipeline by filling the Blanks\ndef pipeline(img):\n    with torch.no_grad():\n        img_var = #Put the Image in PYTorch Variable\n        if torch.cuda.is_available():\n            img_var = # Send to GPU\n        segm, depth = # Call the HydraNet\n        segm = #PostProcess / Resize\n        depth = #PostProcess / Resize\n        segm = #Use the CMAP\n        depth = #Take the Absolute Value\n        return depth, segm","metadata":{"id":"k4jSjgJQU1f0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"depth, segm = pipeline(img)","metadata":{"id":"Id8WbXlQLUm8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,20))\nax1.imshow(img)\nax1.set_title('Original', fontsize=30)\nax2.imshow(segm)\nax2.set_title('Predicted Segmentation', fontsize=30)\nax3.imshow(depth, cmap=\"plasma\", vmin=0, vmax=80)\nax3.set_title(\"Predicted Depth\", fontsize=30)\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":182},"id":"A6M_DJ4rPG1m","outputId":"a91523f0-37fc-47a2-f1e9-ce87add230a6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 ‚Äî Run on a Video","metadata":{"id":"-9309NqKKv0D"}},{"cell_type":"code","source":"print(img.shape)\nprint(depth.shape)\nprint(segm.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Kev1r-xMGwU","outputId":"851741e3-7e1e-4758-8fec-136892cfa3e8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.cm as cm\nimport matplotlib.colors as co\n\ndef depth_to_rgb(depth):\n    normalizer = co.Normalize(vmin=0, vmax=80)\n    mapper = cm.ScalarMappable(norm=normalizer, cmap='plasma')\n    colormapped_im = (mapper.to_rgba(depth)[:, :, :3] * 255).astype(np.uint8)\n    return colormapped_im\n\ndepth_rgb = depth_to_rgb(depth)\nprint(depth_rgb.shape)\nplt.imshow(depth_rgb)\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169},"id":"6OREeqr7WRJF","outputId":"b10d4e10-23fd-4184-842c-f07eae438b88"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(img.shape)\nprint(depth_rgb.shape)\nprint(segm.shape)\nnew_img = np.vstack((img, segm, depth_rgb))\nplt.imshow(new_img)\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":318},"id":"xP2KD52CMnKh","outputId":"5df41906-d2e3-4ba7-febf-d7cb0db11c3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"video_files = sorted(glob.glob(\"data/*.png\"))\n\n# Build a HydraNet\nhydranet = HydraNet()\nhydranet.define_mobilenet()\nhydranet.define_lightweight_refinenet()\nhydranet._initialize_weights()\n\n# Set the Model to Eval on GPU\nif torch.cuda.is_available():\n    _ = hydranet.cuda()\n_ = hydranet.eval()\n\n# Load the Weights\nckpt = torch.load('ExpKITTI_joint.ckpt')\nhydranet.load_state_dict(ckpt['state_dict'])\n\n# Run the pipeline\nresult_video = []\nfor idx, img_path in enumerate(video_files):\n    image = np.array(Image.open(img_path))\n    h, w, _ = image.shape \n    depth, seg = pipeline(image)\n    result_video.append(cv2.cvtColor(cv2.vconcat([image, seg, depth_to_rgb(depth)]), cv2.COLOR_BGR2RGB))\n\nout = cv2.VideoWriter('output/out.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 15, (w,3*h))\n\nfor i in range(len(result_video)):\n    out.write(result_video[i])\nout.release()","metadata":{"id":"l3u7PgoqKuyV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import HTML\nfrom base64 import b64encode\nmp4 = open('output/out.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=800 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)","metadata":{"id":"15-z-lNVxaRX","colab":{"base_uri":"https://localhost:8080/","height":745},"outputId":"23d4c655-c74b-4bbc-9416-736e3caece56"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3D Segmentation\n\nDid you ever wonder... How is segmentation used in self-driving cars? Like, **once you have the map, what do you do with it**?\n<p>\nLet's see something called 3D Segmentation ‚Äî Fusing a Depth Map with a Segmentation Map!\n<p>\n\nIn my course [MASTER STEREO VISION](https://courses.thinkautonomous.ai/stereo-vision), I teach how to do something called **3D Reconstruction** from a Depth Map and Calibration Parameters. <p>\nIn this course, we're going to see how to do it with Open3D, my go-to library for Point Clouds, and we'll see how to build 3D Segmentation Algorithms by fusing the Depth Map (3D) with the Segmentation Map.","metadata":{"id":"dcSVxVHqp2g6"}},{"cell_type":"code","source":"!pip install open3d==0.14.1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"YGmsHUMgsxHT","outputId":"3d50f1ac-1bed-4911-a9b4-c0e29fc51643"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import open3d as o3d","metadata":{"id":"gl4NqJC7sus2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"o3d.__version__","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"BZUSRdrxwPHx","outputId":"41590081-13c3-4749-cd25-46ee8c1a2bee"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RGBD - Fuse the RGB Image and the Depth Map\n\nThe first thing we'll implement is to create an RGBD Image by fusing the RGB Image with the Depth Map. For that, we'll use [Open3D's Class RGBD Image](http://www.open3d.org/docs/release/python_api/open3d.geometry.RGBDImage.html) and the function create_from_color_and_depth(color, depth).<p>\nIt looks pretty straghtforward, we just need to make sure that the image are loaded as [Open3D Images](http://www.open3d.org/docs/release/python_api/open3d.geometry.Geometry.html?highlight=image#open3d.geometry.Geometry.Image).","metadata":{"id":"a0MwLXd-1Lce"}},{"cell_type":"code","source":"rgbd = #TODO: Call the Function","metadata":{"id":"CduFT43RUaxH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we'll use the function create_from_rgbd_image to build a Point Cloud based on this. For that, we'll need the camera's intrinsic parameters. <p>\nIf you'd like to learn more about this, I invite you to take my course on [Stereo Vision](https://courses.thinkautonomous.ai/stereo-vision). In this course, I'm just going to give'em to you.","metadata":{"id":"osFtnlbzUbp-"}},{"cell_type":"code","source":"o3d.camera.PinholeCameraIntrinsic??","metadata":{"id":"3n9Wuq3fvr5y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"intrinsics = o3d.camera.PinholeCameraIntrinsic(width = 1242, height = 375, fx = 721., fy = 721., cx = 609., cy = 609.)","metadata":{"id":"NZWxv0-Htine"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"point_cloud = #TODO: Create A Point Cloud\no3d.io.write_point_cloud(\"test.pcd\", point_cloud)","metadata":{"id":"m6rpNbLSTon4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e59c16dc-9315-41e9-96c1-e815cc3880d9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3D Segmentation ‚Äî Fuse the Segmentation Map with the Depth Map\nFrom now on, the process is exactly the same. But instead of creating a Point Cloud from an RGBD Image with the Normal RGB Image, we'll do it with the Depth Map.","metadata":{"id":"dXkvW7hA1Nzl"}},{"cell_type":"code","source":"rgbd = #TODO: Call the Function","metadata":{"id":"8Vt6VWphzsDz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"point_cloud = #TODO: Create A Point Cloud","metadata":{"id":"HDw31-VR0HwC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"o3d.io.write_point_cloud(\"test_segm.pcd\", point_cloud)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IR_UrU-i0wLV","outputId":"0fef3015-2b67-4fd1-d269-402dc1af749e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"UleBi3tUeAST"},"execution_count":null,"outputs":[]}]}