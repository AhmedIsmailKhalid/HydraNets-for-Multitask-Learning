{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# HydraNet for Self-Driving Cars\n","In this notebook, you're going to learn how to build a Neural Network that has:\n","* Input: **a monocular RGB Image**\n","* Output: **a Depth Map**, and **a Segmentation Map**\n","\n","A single model, two different outputs. For that, out model will need to use a principle called Multi Task Learning.<p>"],"metadata":{"id":"20Fk1u9m9xfl"}},{"cell_type":"markdown","source":["# 1 - Imports"],"metadata":{"id":"5Ix2A28T-ZT_"}},{"cell_type":"code","source":["!pip install -U tensorflow"],"metadata":{"execution":{"iopub.status.busy":"2022-12-21T20:42:36.487806Z","iopub.execute_input":"2022-12-21T20:42:36.488321Z","iopub.status.idle":"2022-12-21T20:44:03.535129Z","shell.execute_reply.started":"2022-12-21T20:42:36.488274Z","shell.execute_reply":"2022-12-21T20:44:03.533891Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"jVuo5WLvqbUh","executionInfo":{"status":"ok","timestamp":1671757078684,"user_tz":300,"elapsed":100794,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}},"outputId":"e1852e54-c98a-4f11-cb26-62469df1b5ca"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n","Collecting tensorflow\n","  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n","\u001b[K     |████████████████████████████████| 588.3 MB 18 kB/s \n","\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n","  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n","\u001b[K     |████████████████████████████████| 439 kB 40.4 MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.28.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n","Collecting keras<2.12,>=2.11.0\n","  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 48.0 MB/s \n","\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n","Collecting tensorboard<2.12,>=2.11\n","  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n","\u001b[K     |████████████████████████████████| 6.0 MB 51.6 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.1.1)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n","Collecting flatbuffers>=2.0\n","  Downloading flatbuffers-22.12.6-py2.py3-none-any.whl (26 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.15.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.23.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (5.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.11.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n","Installing collected packages: tensorflow-estimator, tensorboard, keras, flatbuffers, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.9.0\n","    Uninstalling tensorflow-estimator-2.9.0:\n","      Successfully uninstalled tensorflow-estimator-2.9.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.9.1\n","    Uninstalling tensorboard-2.9.1:\n","      Successfully uninstalled tensorboard-2.9.1\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.9.0\n","    Uninstalling keras-2.9.0:\n","      Successfully uninstalled keras-2.9.0\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 1.12\n","    Uninstalling flatbuffers-1.12:\n","      Successfully uninstalled flatbuffers-1.12\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.9.2\n","    Uninstalling tensorflow-2.9.2:\n","      Successfully uninstalled tensorflow-2.9.2\n","Successfully installed flatbuffers-22.12.6 keras-2.11.0 tensorboard-2.11.0 tensorflow-2.11.0 tensorflow-estimator-2.11.0\n"]}]},{"cell_type":"code","source":["!wget https://hydranets-data.s3.eu-west-3.amazonaws.com/hydranets-data.zip && unzip -q hydranets-data.zip && mv hydranets-data/* . && rm hydranets-data.zip && rm -rf hydranets-data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wnk8xflh-RfW","outputId":"82bd9ef1-4de5-4136-bc58-d5e90e4c03b8","execution":{"iopub.status.busy":"2022-12-21T20:44:03.537579Z","iopub.execute_input":"2022-12-21T20:44:03.537919Z","iopub.status.idle":"2022-12-21T20:44:07.509067Z","shell.execute_reply.started":"2022-12-21T20:44:03.537886Z","shell.execute_reply":"2022-12-21T20:44:07.507853Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1671757087171,"user_tz":300,"elapsed":8499,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-12-23 00:57:58--  https://hydranets-data.s3.eu-west-3.amazonaws.com/hydranets-data.zip\n","Resolving hydranets-data.s3.eu-west-3.amazonaws.com (hydranets-data.s3.eu-west-3.amazonaws.com)... 52.95.156.44\n","Connecting to hydranets-data.s3.eu-west-3.amazonaws.com (hydranets-data.s3.eu-west-3.amazonaws.com)|52.95.156.44|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 110752264 (106M) [application/zip]\n","Saving to: ‘hydranets-data.zip’\n","\n","hydranets-data.zip  100%[===================>] 105.62M  25.9MB/s    in 4.8s    \n","\n","2022-12-23 00:58:04 (22.2 MB/s) - ‘hydranets-data.zip’ saved [110752264/110752264]\n","\n"]}]},{"cell_type":"code","source":["# Install the tensorflow-addons and onnx-tensorflow\n","!git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow && pip install -e .\n","!pip install tensorflow-addons"],"metadata":{"execution":{"iopub.status.busy":"2022-12-21T20:44:07.510632Z","iopub.execute_input":"2022-12-21T20:44:07.510999Z","iopub.status.idle":"2022-12-21T20:44:36.722852Z","shell.execute_reply.started":"2022-12-21T20:44:07.510962Z","shell.execute_reply":"2022-12-21T20:44:36.720873Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"KP1zgtxoqbUj","executionInfo":{"status":"ok","timestamp":1671757138825,"user_tz":300,"elapsed":51669,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}},"outputId":"a657a62b-003d-4262-9972-c1a83f34bdf7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'onnx-tensorflow'...\n","remote: Enumerating objects: 6516, done.\u001b[K\n","remote: Counting objects: 100% (465/465), done.\u001b[K\n","remote: Compressing objects: 100% (202/202), done.\u001b[K\n","remote: Total 6516 (delta 323), reused 380 (delta 259), pack-reused 6051\u001b[K\n","Receiving objects: 100% (6516/6516), 1.98 MiB | 15.12 MiB/s, done.\n","Resolving deltas: 100% (5050/5050), done.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining file:///content/onnx-tensorflow\n","Collecting onnx>=1.10.2\n","  Downloading onnx-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n","\u001b[K     |████████████████████████████████| 13.5 MB 12.4 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from onnx-tf==1.10.0) (6.0)\n","Collecting tensorflow_addons\n","  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 13.1 MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow_probability in /usr/local/lib/python3.8/dist-packages (from onnx-tf==1.10.0) (0.17.0)\n","Collecting protobuf<4,>=3.20.2\n","  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 52.5 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.10.2->onnx-tf==1.10.0) (4.4.0)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.10.2->onnx-tf==1.10.0) (1.21.6)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons->onnx-tf==1.10.0) (2.7.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons->onnx-tf==1.10.0) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow_addons->onnx-tf==1.10.0) (3.0.9)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability->onnx-tf==1.10.0) (0.1.7)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability->onnx-tf==1.10.0) (1.5.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability->onnx-tf==1.10.0) (4.4.2)\n","Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability->onnx-tf==1.10.0) (0.4.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability->onnx-tf==1.10.0) (1.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability->onnx-tf==1.10.0) (1.3.0)\n","Installing collected packages: protobuf, tensorflow-addons, onnx, onnx-tf\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.19.6\n","    Uninstalling protobuf-3.19.6:\n","      Successfully uninstalled protobuf-3.19.6\n","  Running setup.py develop for onnx-tf\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\n","Successfully installed onnx-1.13.0 onnx-tf-1.10.0 protobuf-3.20.3 tensorflow-addons-0.19.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.8/dist-packages (0.19.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (21.3)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OKJs4GzBqzqe","executionInfo":{"status":"ok","timestamp":1671757205804,"user_tz":300,"elapsed":66990,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}},"outputId":"f5e8327f-089b-4f61-f564-c5be4fe7a097"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%matplotlib inline\n","import sys\n","sys.path.append(\"./onnx-tensorflow\")\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","import math\n","import onnx\n","from onnx_tf.backend import prepare"],"metadata":{"id":"szGHb1YQSyJ_","execution":{"iopub.status.busy":"2022-12-21T20:44:36.727384Z","iopub.execute_input":"2022-12-21T20:44:36.727827Z","iopub.status.idle":"2022-12-21T20:44:44.403772Z","shell.execute_reply.started":"2022-12-21T20:44:36.727781Z","shell.execute_reply":"2022-12-21T20:44:44.402551Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1671757222583,"user_tz":300,"elapsed":16789,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# 2 — Creating the HydraNet\n","We now have 2 DataLoaders: one for training, and one for validation/test. <p>\n","\n","In the next step, we're going to define our model, following the paper [Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations](https://arxiv.org/pdf/1809.04766.pdf) —— If you haven't read it yet, now is the time.<p>\n","\n","A Note — This notebook has been adapted from DrSleep, a researcher named Vladimir, who authorized me to adapt it for education purposes. [Here's the notebook I'm refering to](https://github.com/DrSleep/multi-task-refinenet/blob/master/src/notebooks/ExpNYUDKITTI_joint.ipynb/).\n","\n","<p>\n","\n","> ![](https://d3i71xaburhd42.cloudfront.net/435d4b5c30f10753d277848a17baddebd98d3c31/2-Figure1-1.png)\n","\n","Our model takes an input RGB image, make it go through an encoder, a lightweight refinenet decoder, and then has 2 heads, one for each task.<p>\n","Things to note:\n","* The only **convolutions** we'll need will be 3x3 and 1x1\n","* We also need a **MaxPooling 5x5**\n","* **CRP-Blocks** are implemented as Skip-Connection Operations\n","* **Each Head is made of a 1x1 convolution followed by a 3x3 convolution**, only the data and the loss change there\n"],"metadata":{"id":"LuR-bk-13cX6"}},{"cell_type":"markdown","source":["## 2.1 — Create a HydraNet class"],"metadata":{"id":"Pf8671mIE_tU"}},{"cell_type":"markdown","source":["```\n","class HydraNet(tf.keras.Model):\n","    def __init__(self):        \n","        super(HydraNet, self).__init__() # Python 3\n","        self.num_tasks = 2\n","        self.num_classes = 6\n","```"],"metadata":{"execution":{"iopub.status.busy":"2022-12-21T04:01:19.607977Z","iopub.execute_input":"2022-12-21T04:01:19.608456Z","iopub.status.idle":"2022-12-21T04:01:19.613721Z","shell.execute_reply.started":"2022-12-21T04:01:19.608417Z","shell.execute_reply":"2022-12-21T04:01:19.612496Z"},"id":"AKcoI1iAqbUm"}},{"cell_type":"markdown","source":["```\n","net = HydraNet()\n","```"],"metadata":{"execution":{"iopub.status.busy":"2022-12-21T04:01:22.235550Z","iopub.execute_input":"2022-12-21T04:01:22.236103Z","iopub.status.idle":"2022-12-21T04:01:22.241955Z","shell.execute_reply.started":"2022-12-21T04:01:22.236061Z","shell.execute_reply":"2022-12-21T04:01:22.240460Z"},"id":"CjVsZl53qbUm"}},{"cell_type":"markdown","source":["```\n","Layer(1) S1\n","    conv2d(32, k=3, s=2, padding=1, bias=False)\n","    batchnorm(eps=1e-05, momentum=0.1)\n","    relu(6) \n","    \n","Layer(2) IRB\n","    conv2d(32, k=1, s=1, bias=False)\n","    batchnorm(eps=1e-05, momentum=0.1)\n","    relu(6) \n","    \n","    \n","```    "],"metadata":{"execution":{"iopub.status.busy":"2022-12-17T01:53:05.397541Z","iopub.execute_input":"2022-12-17T01:53:05.398003Z","iopub.status.idle":"2022-12-17T01:53:05.405818Z","shell.execute_reply.started":"2022-12-17T01:53:05.397970Z","shell.execute_reply":"2022-12-17T01:53:05.404133Z"},"id":"bBJ_4aKqqbUm"}},{"cell_type":"markdown","source":["## 2.2 — Defining the Encoder: A MobileNetv2\n","![](https://iq.opengenus.org/content/images/2020/11/conv_mobilenet_v2.jpg)"],"metadata":{"id":"025K4utrBqNy"}},{"cell_type":"code","source":["def conv3x3(filters, stride=1, bias=False, dilation=1, groups=1):\n","    # 3x3 convolution\n","    return tf.keras.layers.Conv2D(filters, kernel_size=3, strides=stride,\n","                     padding='same', dilation_rate=dilation, use_bias=bias, groups=groups)"],"metadata":{"id":"TpCvdaopBGqo","execution":{"iopub.status.busy":"2022-12-21T20:44:44.405251Z","iopub.execute_input":"2022-12-21T20:44:44.405933Z","iopub.status.idle":"2022-12-21T20:44:44.411940Z","shell.execute_reply.started":"2022-12-21T20:44:44.405894Z","shell.execute_reply":"2022-12-21T20:44:44.411082Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1671757223084,"user_tz":300,"elapsed":506,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Test conv3x3\n","conv3x3(filters=32)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-21T20:44:44.413215Z","iopub.execute_input":"2022-12-21T20:44:44.413576Z","iopub.status.idle":"2022-12-21T20:44:44.451506Z","shell.execute_reply.started":"2022-12-21T20:44:44.413543Z","shell.execute_reply":"2022-12-21T20:44:44.450192Z"},"trusted":true,"id":"g3AP1kxyqbUo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671757223195,"user_tz":300,"elapsed":115,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}},"outputId":"e2896f10-ffbc-46b2-ba95-b37c13629724"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<keras.layers.convolutional.conv2d.Conv2D at 0x7f811e0535e0>"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["def conv1x1(filters, stride=1, bias=False, groups=1):\n","    # 1x1 convolution\n","    return tf.keras.layers.Conv2D(filters, kernel_size=1, strides=stride,\n","                     padding='valid', use_bias=bias, groups=groups)"],"metadata":{"id":"X26XhiFLBZD5","execution":{"iopub.status.busy":"2022-12-21T20:44:44.453300Z","iopub.execute_input":"2022-12-21T20:44:44.454318Z","iopub.status.idle":"2022-12-21T20:44:44.461372Z","shell.execute_reply.started":"2022-12-21T20:44:44.454255Z","shell.execute_reply":"2022-12-21T20:44:44.460175Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1671757223196,"user_tz":300,"elapsed":9,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Test conv1x1\n","conv1x1(filters=32)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-21T20:44:44.462958Z","iopub.execute_input":"2022-12-21T20:44:44.463393Z","iopub.status.idle":"2022-12-21T20:44:44.476060Z","shell.execute_reply.started":"2022-12-21T20:44:44.463358Z","shell.execute_reply":"2022-12-21T20:44:44.475041Z"},"trusted":true,"id":"kGIAEnueqbUo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671757223196,"user_tz":300,"elapsed":8,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}},"outputId":"8eb0a628-12b7-4e9d-f642-60ea62255895"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<keras.layers.convolutional.conv2d.Conv2D at 0x7f80a4a38850>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["def batchnorm():\n","    # batch norm 2d\n","    batch_norm = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1)\n","    batch_norm.trainable = True\n","    return batch_norm"],"metadata":{"execution":{"iopub.status.busy":"2022-12-21T20:44:44.477822Z","iopub.execute_input":"2022-12-21T20:44:44.478196Z","iopub.status.idle":"2022-12-21T20:44:44.487531Z","shell.execute_reply.started":"2022-12-21T20:44:44.478162Z","shell.execute_reply":"2022-12-21T20:44:44.486344Z"},"trusted":true,"id":"W14xf6PoqbUp","executionInfo":{"status":"ok","timestamp":1671757223197,"user_tz":300,"elapsed":7,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Test batchnorm\n","batchnorm()"],"metadata":{"execution":{"iopub.status.busy":"2022-12-21T20:44:44.494710Z","iopub.execute_input":"2022-12-21T20:44:44.495264Z","iopub.status.idle":"2022-12-21T20:44:44.511427Z","shell.execute_reply.started":"2022-12-21T20:44:44.495151Z","shell.execute_reply":"2022-12-21T20:44:44.510266Z"},"trusted":true,"id":"vRQmc8GCqbUp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671757223293,"user_tz":300,"elapsed":102,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}},"outputId":"812c28ff-0679-4e25-b8ab-55687e57cf9e"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f80a4158dc0>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["def convbnrelu(filters, kernel_size, stride=1, groups=1, act=True):\n","    # conv-batchnorm-relu\n","    if int(kernel_size/2) == 1 :\n","        padding = 'same'\n","    if int(kernel_size/2) == 0 :\n","        padding = 'valid'\n","    if act:\n","        return tf.keras.Sequential([tf.keras.layers.Conv2D(filters, kernel_size, strides=stride, padding=padding, groups=groups, use_bias=False),\n","                             batchnorm(),\n","                             tf.keras.layers.ReLU(max_value=6)])\n","    else:\n","        return tf.keras.Sequential([tf.keras.layers.Conv2D(filters, kernel_size, strides=stride, padding=padding, groups=groups, use_bias=False),\n","                             batchnorm()])"],"metadata":{"id":"ytAJff2_QJym","execution":{"iopub.status.busy":"2022-12-21T20:44:44.514236Z","iopub.execute_input":"2022-12-21T20:44:44.515226Z","iopub.status.idle":"2022-12-21T20:44:44.524174Z","shell.execute_reply.started":"2022-12-21T20:44:44.515185Z","shell.execute_reply":"2022-12-21T20:44:44.523191Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1671757223422,"user_tz":300,"elapsed":131,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Test convbnrelu\n","display(convbnrelu(32,3,1,1,True).layers)\n","print()\n","display(convbnrelu(32,3,1,1,False).layers)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-21T20:44:44.526449Z","iopub.execute_input":"2022-12-21T20:44:44.527341Z","iopub.status.idle":"2022-12-21T20:44:44.594836Z","shell.execute_reply.started":"2022-12-21T20:44:44.527292Z","shell.execute_reply":"2022-12-21T20:44:44.593776Z"},"trusted":true,"id":"basLcMXWqbUp","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1671757224129,"user_tz":300,"elapsed":709,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}},"outputId":"f5242f16-1554-4ef2-bd7b-b1c44c2cd30c"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":["[<keras.layers.convolutional.conv2d.Conv2D at 0x7f811e053dc0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f811e076ac0>,\n"," <keras.layers.activation.relu.ReLU at 0x7f80a4b1bac0>]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["[<keras.layers.convolutional.conv2d.Conv2D at 0x7f811e053dc0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f811e076d90>]"]},"metadata":{}}]},{"cell_type":"code","source":["class InvertedResidualBlock(tf.keras.Model) :\n","    def __init__(self,in_planes, filters, expansion_factor, stride) :\n","        super(InvertedResidualBlock, self).__init__()\n","        intermed_planes = in_planes * expansion_factor\n","        self.residual = (in_planes == filters) and (stride == 1) # Boolean/Condition\n","        self.IBR = tf.keras.Sequential([convbnrelu(in_planes, kernel_size=1, stride=stride, act=True), \n","                               convbnrelu(intermed_planes, kernel_size=3, \n","                                          stride=stride, groups=intermed_planes, act=True), \n","                               convbnrelu(filters, kernel_size=1, stride=stride, act=False)])\n","        \n","    def call(self, inputs) :\n","        x = self.IBR(inputs)\n","        if self.residual :\n","            return (x + inputs)\n","        else :\n","            return x"],"metadata":{"execution":{"iopub.status.busy":"2022-12-21T20:44:44.596574Z","iopub.execute_input":"2022-12-21T20:44:44.596910Z","iopub.status.idle":"2022-12-21T20:44:44.605234Z","shell.execute_reply.started":"2022-12-21T20:44:44.596880Z","shell.execute_reply":"2022-12-21T20:44:44.603529Z"},"trusted":true,"id":"qEtIuESkqbUq","executionInfo":{"status":"ok","timestamp":1671757224130,"user_tz":300,"elapsed":7,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# class MobileNetV2(tf.keras.Model) :\n","#     def __init__(self):        \n","#         super(MobileNetV2, self).__init__()\n","\n","#         self.LAYERS=[]\n","#         mobilenet_config = [[1, 16, 1, 1], # expansion rate, output channels, number of repeats, stride\n","#                         [6, 24, 2, 2],\n","#                         [6, 32, 3, 2],\n","#                         [6, 64, 4, 2],\n","#                         [6, 96, 3, 1],\n","#                         [6, 160, 3, 2],\n","#                         [6, 320, 1, 1],\n","#                         ]\n","#         self.in_channels = 32 # number of input channels\n","#         self.num_layers = len(mobilenet_config)\n","#         self.layer1 = convbnrelu(filters=32, kernel_size=3, stride=2) # This is the first layer of the first \n","#         layer1_model = tf.keras.Sequential(self.layer1)\n","#         layer1_model._name = 'layer1'\n","#         self.LAYERS.append(layer1_model)\n","\n","#         c_layer = 2\n","#         for t,c,n,s in (mobilenet_config):\n","#             layers = []\n","\n","#             for idx in range(n):\n","#                 layers.append(InvertedResidualBlock(self.in_channels, c, expansion_factor=t, stride=s if idx == 0 else 1))\n","#                 self.in_channels = c\n","#             self.LAYERS.append(tf.keras.Sequential(layers))\n","#             c_layer += 1\n","\n","# #         for model, i in zip(LAYERS[1:], range(2,9)):\n","# #             model._name = f'layer{i}'\n","\n","#     def call(self, inputs) :\n","#         l1 = self.LAYERS[0](inputs)\n","#         l2 = self.LAYERS[1](l1)\n","#         l3 = self.LAYERS[2](l2)\n","#         l4 = self.LAYERS[3](l3)\n","#         l5 = self.LAYERS[4](l4)\n","#         l6 = self.LAYERS[5](l5)\n","#         l7 = self.LAYERS[6](l6)\n","#         l8 = self.LAYERS[7](l7)\n","        \n","#         return l3, l4, l5, l6, l7, l8\n","\n","class MobileNetv2(tf.keras.Model):\n","    def __init__(self, return_idx=[6]):\n","        super().__init__()\n","        # expansion rate, output channels, number of repeats, stride\n","        self.mobilenet_config = [\n","        [1, 16, 1, 1],\n","        [6, 24, 2, 2],\n","        [6, 32, 3, 2],\n","        [6, 64, 4, 2],\n","        [6, 96, 3, 1],\n","        [6, 160, 3, 2],\n","        [6, 320, 1, 1],\n","        ]\n","        self.in_channels = 32  # number of input channels\n","        self.num_layers = len(self.mobilenet_config)\n","        self.layer1 = convbnrelu(self.in_channels, kernel_size=3, stride=2)\n","    \n","        self.return_idx = [1, 2, 3, 4, 5, 6]\n","        #self.return_idx = make_list(return_idx)\n","\n","        c_layer = 2\n","        for t, c, n, s in self.mobilenet_config:\n","            layers = []\n","            for idx in range(n):\n","                layers.append(InvertedResidualBlock(self.in_channels,c,expansion_factor=t,stride=s if idx == 0 else 1,))\n","                self.in_channels = c\n","            setattr(self, \"layer{}\".format(c_layer), tf.keras.Sequential(layers))\n","            c_layer += 1\n","\n","        self._out_c = [self.mobilenet_config[idx][1] for idx in self.return_idx] # Output: [24, 32, 64, 96, 160, 320]\n","\n","    def call(self, x):\n","        outs = []\n","        x = self.layer1(x)\n","        outs.append(self.layer2(x))  # 16, x / 2\n","        outs.append(self.layer3(outs[-1]))  # 24, x / 4\n","        outs.append(self.layer4(outs[-1]))  # 32, x / 8\n","        outs.append(self.layer5(outs[-1]))  # 64, x / 16\n","        outs.append(self.layer6(outs[-1]))  # 96, x / 16\n","        outs.append(self.layer7(outs[-1]))  # 160, x / 32\n","        outs.append(self.layer8(outs[-1]))  # 320, x / 32\n","        return [outs[idx] for idx in self.return_idx]\n","     "],"metadata":{"execution":{"iopub.status.busy":"2022-12-21T20:44:44.606905Z","iopub.execute_input":"2022-12-21T20:44:44.607915Z","iopub.status.idle":"2022-12-21T20:44:44.622002Z","shell.execute_reply.started":"2022-12-21T20:44:44.607876Z","shell.execute_reply":"2022-12-21T20:44:44.620981Z"},"trusted":true,"id":"CtSxsX8LqbUq","executionInfo":{"status":"ok","timestamp":1671757224130,"user_tz":300,"elapsed":6,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["## 2.3 — Defining the Decoder - A Multi-Task Lighweight RefineNet\n","Paper: https://arxiv.org/pdf/1810.03272.pdf\n","\n","![](https://d3i71xaburhd42.cloudfront.net/4d653b19ce1c7cba79fc2f11271fb90f7744c95c/4-Figure1-1.png)"],"metadata":{"id":"ymQKYmnjCEng"}},{"cell_type":"code","source":["class CRPBlock(tf.keras.Model):\n","    \"\"\"CRP definition\"\"\"\n","    def __init__(self, in_planes, out_planes, n_stages, groups=False):\n","        super(CRPBlock, self).__init__() #Python 3\n","        for i in range(n_stages):\n","            setattr(self, '{}_{}'.format(i + 1, 'outvar_dimred'),\n","                    conv1x1(out_planes, stride=1,\n","                            bias=False, groups=in_planes if groups else 1)) #setattr(object, name, value)\n","\n","        self.stride = 1\n","        self.n_stages = n_stages\n","        self.maxpool = tf.keras.layers.MaxPool2D(pool_size=5, strides=1, padding='same')\n","\n","    def call(self, inputs):\n","        top = inputs\n","        for i in range(self.n_stages):\n","            top = self.maxpool(top)\n","            top = getattr(self, '{}_{}'.format(i + 1, 'outvar_dimred'))(top)#getattr(object, name[, default])\n","            inputs = top + inputs\n","        return inputs"],"metadata":{"id":"N6Ia8Cm2BhN6","execution":{"iopub.status.busy":"2022-12-21T20:44:44.624980Z","iopub.execute_input":"2022-12-21T20:44:44.625831Z","iopub.status.idle":"2022-12-21T20:44:44.637235Z","shell.execute_reply.started":"2022-12-21T20:44:44.625783Z","shell.execute_reply":"2022-12-21T20:44:44.636279Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1671757224276,"user_tz":300,"elapsed":152,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["class LightweightRefineNet(tf.keras.Model):\n","    def __init__(self, num_tasks, num_classes) :\n","        super(LightweightRefineNet, self).__init__() \n","        \n","        self.num_tasks = num_tasks\n","        self.num_classes = num_classes\n","        \n","        self.conv8 = conv1x1(256, bias=False)\n","        self.conv7 = conv1x1(256, bias=False)\n","        self.conv6 = conv1x1(256, bias=False)\n","        self.conv5 = conv1x1(256, bias=False)\n","        self.conv4 = conv1x1(256, bias=False)\n","        self.conv3 = conv1x1(256, bias=False)\n","        self.crp4 = self._make_crp(256, 256, 4, groups=False)\n","        self.crp3 = self._make_crp(256, 256, 4, groups=False)\n","        self.crp2 = self._make_crp(256, 256, 4, groups=False)\n","        self.crp1 = self._make_crp(256, 256, 4, groups=True)\n","\n","        self.conv_adapt4 = conv1x1(256, bias=False)\n","        self.conv_adapt3 = conv1x1(256, bias=False)\n","        self.conv_adapt2 = conv1x1(256, bias=False)\n","\n","        self.pre_depth = conv1x1(256, groups=256, bias=False)\n","        self.depth = conv3x3(1, bias=True)\n","        self.pre_segm = conv1x1(256, groups=256, bias=False)\n","        self.segm = conv3x3(self.num_classes, bias=True)\n","        self.relu = tf.keras.layers.ReLU(6)\n","\n","        if self.num_tasks == 3:\n","            self.pre_normal = conv1x1(256, groups=256, bias=False)\n","            self.normal = conv3x3(3, bias=True)\n","                                 \n","                                 \n","    def _make_crp(self, in_planes, out_planes, stages, groups=False):\n","        layers = [CRPBlock(in_planes, out_planes,stages, groups=groups)]\n","        return tf.keras.Sequential(layers)\n","    \n","    \n","    def call(self, l3, l4, l5, l6, l7, l8) :\n","        l8 = self.conv8(l8)\n","        l7 = self.conv7(l7)\n","        l7 = self.relu(l8+l7)\n","        l7 = self.crp4(l7)\n","        l7 = self.conv_adapt4(l7)\n","        l7 = tf.keras.layers.UnSampling2D(size = l6.size()[2:],mode='bilinear', align_corners=False)(l7)\n","\n","        l6 = self.conv6(l6)\n","        l5 = self.conv5(l5)\n","        l5 = self.relu(l5+l6+l7)\n","        l5 = self.crp3(l5)\n","        l5 = self.conv_adapt3(l5)\n","        l5 = tf.keras.layers.UnSampling2D(size = l4.size()[2:],mode='bilinear', align_corners=False)(l5)\n","        l4 = self.conv4(l4)\n","        l4 = self.relu(l5+l4)\n","        l4 = self.crp2(l4)\n","        l4 = self.conv_adapt2(l4)\n","        l4 = tf.keras.layers.UnSampling2D(size=l3.size()[2:],mode='bilinear', align_corners=False)(l4)\n","\n","        l3 = self.conv3(l3)\n","        l3 = self.relu(l3+l4)\n","        l3 = self.crp1(l3)\n","\n","        out_segm = self.pre_segm(l3)\n","        out_segm = self.relu(out_segm)\n","        out_segm = self.segm(out_segm)\n","\n","        out_depth = self.pre_depth(l3)\n","        out_depth = self.relu(out_depth)\n","        out_depth = self.depth(out_depth)\n","        \n","        if self.num_tasks == 3:\n","            out_n = self.pre_normal(l3)\n","            out_n = self.relu(out_n)\n","            out_n = self.normal(out_n)\n","            return out_segm, out_depth, out_n\n","        \n","        else:\n","            return out_segm, out_depth\n"],"metadata":{"execution":{"iopub.status.busy":"2022-12-21T20:44:44.640760Z","iopub.execute_input":"2022-12-21T20:44:44.641245Z","iopub.status.idle":"2022-12-21T20:44:44.664337Z","shell.execute_reply.started":"2022-12-21T20:44:44.641090Z","shell.execute_reply":"2022-12-21T20:44:44.662907Z"},"trusted":true,"id":"07Gf5cm_qbUs","executionInfo":{"status":"ok","timestamp":1671757224437,"user_tz":300,"elapsed":163,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["## 2.4 — Define the HydraNet Forward Function\n","\n","> ![](https://d3i71xaburhd42.cloudfront.net/435d4b5c30f10753d277848a17baddebd98d3c31/2-Figure1-1.png)"],"metadata":{"id":"XdFV4osfIZNc"}},{"cell_type":"markdown","source":["# 3 — Run the Model"],"metadata":{"id":"3KwKZoWT--AD"}},{"cell_type":"markdown","source":["## 3.1 — Load the Model Weights"],"metadata":{"id":"aXGQHCwtI87U"}},{"cell_type":"code","source":["# if torch.cuda.is_available():\n","#     _ = hydranet.cuda()\n","# _ = hydranet.eval()"],"metadata":{"id":"Aty_yRuOTlx_","execution":{"iopub.status.busy":"2022-12-21T20:44:44.666193Z","iopub.execute_input":"2022-12-21T20:44:44.666611Z","iopub.status.idle":"2022-12-21T20:44:44.677151Z","shell.execute_reply.started":"2022-12-21T20:44:44.666579Z","shell.execute_reply":"2022-12-21T20:44:44.676003Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1671757224438,"user_tz":300,"elapsed":4,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["class HydraNet(tf.keras.Model):\n","    def __init__(self, num_tasks, num_classes) :\n","        super(HydraNet, self).__init__()\n","        \n","        self.num_tasks = num_tasks\n","        self.num_classes = num_classes\n","\n","        self.encoder = MobileNetv2()\n","        self.decoder = LightweightRefineNet(num_tasks,num_classes)\n","        \n","    def call(self, inputs) :\n","        l3, l4, l5, l6, l7, l8 = self.encoder(inputs)\n","        if self.num_tasks == 3 :\n","            out_depth, out_segm, out_n = self.decoder(l3, l4, l5, l6, l7, l8)\n","            return out_depth, out_segm, out_n\n","        else :\n","            out_depth, out_segm = self.decoder(l3, l4, l5, l6, l7, l8)\n","            return out_depth, out_segm"],"metadata":{"execution":{"iopub.status.busy":"2022-12-21T20:44:44.678618Z","iopub.execute_input":"2022-12-21T20:44:44.679355Z","iopub.status.idle":"2022-12-21T20:44:44.689598Z","shell.execute_reply.started":"2022-12-21T20:44:44.679321Z","shell.execute_reply":"2022-12-21T20:44:44.688463Z"},"trusted":true,"id":"9Liu7eDVqbUu","executionInfo":{"status":"ok","timestamp":1671757224438,"user_tz":300,"elapsed":4,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["hydranet = HydraNet(6,2)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-21T20:44:44.690953Z","iopub.execute_input":"2022-12-21T20:44:44.691437Z","iopub.status.idle":"2022-12-21T20:44:45.235118Z","shell.execute_reply.started":"2022-12-21T20:44:44.691392Z","shell.execute_reply":"2022-12-21T20:44:45.233875Z"},"trusted":true,"id":"BzsbNZbmqbUu","executionInfo":{"status":"ok","timestamp":1671757551091,"user_tz":300,"elapsed":1379,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["hydranet.compile(optimizer=[tf.keras.optimizers.SGD(learning_rate=1e-2, momentum=0.9, weight_decay=1e-5), \n","                            tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9, weight_decay=1e-5)],\n","                 loss=[tf.keras.losses.SparseCategoricalCrossentropy(), tf.keras.losses.Huber()])\n"],"metadata":{"id":"dhd_p6qGg9es","executionInfo":{"status":"ok","timestamp":1671758420779,"user_tz":300,"elapsed":148,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["hydranet.fit(train)"],"metadata":{"id":"-2Dtepg2kSwd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["|# import onnx\n","# from onnx_tf.backend import prepare\n"," \n","# onnx_model = onnx.load(\"/content/drive/MyDrive/Colab Notebooks/HydraNets/KITTI/TensorFlow/ExpKITTI_joint.onnx\")\n","# tf_rep = prepare(onnx_model)\n","# tf_rep.export_graph(\"model.pb\")"],"metadata":{"id":"VVuzMCd3ONqQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# img = np.asarray(Image.open('/content/data/0000000000.png'), dtype=np.float32).transpose(2, 0, 1)[None]\n","# tf_rep.run(img)"],"metadata":{"id":"NrPtu9K_IIC3","executionInfo":{"status":"ok","timestamp":1671757258734,"user_tz":300,"elapsed":133,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# model = tf.saved_model.load('/content/model.pb')\n","# infer = model.signatures['serving_default']\n","\n","# img_var = tf.convert_to_tensor(prepare_img(img).transpose(2, 0, 1)[None])\n","# img_var = tf.cast(img_var, tf.float32)"],"metadata":{"id":"WyYMbkDvoto_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# outputs = list(infer.structured_outputs)\n","# y = infer(img_var)[outputs[0]].numpy()\n","# # print(y)\n","# outputs"],"metadata":{"id":"M7LgSUeSrFv1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !rm -rf '/content/model'#, '/content/model.pb'"],"metadata":{"id":"iufJ2zTcV5Jc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.2 — Preprocess Images"],"metadata":{"id":"DaRUKyEFKUQy"}},{"cell_type":"code","source":["IMG_SCALE  = 1./255\n","IMG_MEAN = np.array([0.485, 0.456, 0.406]).reshape((1, 1, 3))\n","IMG_STD = np.array([0.229, 0.224, 0.225]).reshape((1, 1, 3))\n","\n","def prepare_img(img):\n","    return (img * IMG_SCALE - IMG_MEAN) / IMG_STD"],"metadata":{"id":"7fOiKvtWUQkf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.3 — Load and Run an Image"],"metadata":{"id":"ZEnd98Y9KWsZ"}},{"cell_type":"code","source":["# Pre-processing and post-processing constants #\n","CMAP = np.load('cmap_kitti.npy')\n","NUM_CLASSES = 6"],"metadata":{"id":"6kWsQzZWTKvW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(CMAP)"],"metadata":{"id":"ZTwp5nMcbtC7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import glob\n","images_files = glob.glob('data/*.png')\n","idx = np.random.randint(0, len(images_files))\n","\n","img_path = images_files[idx]\n","img = np.array(Image.open(img_path))\n","plt.imshow(img)\n","plt.show()"],"metadata":{"id":"Qk1OkvnvUfyy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pipeline(img):\n","    img_var = tf.Variable(tf.convert_to_tensor(prepare_img(img).transpose(2, 0, 1)[None]))\n","    segm, depth = model(img_var)\n","    segm = cv2.resize(segm[0, :NUM_CLASSES].data.numpy().transpose(1, 2, 0),\n","                    img.shape[:2][::-1],\n","                    interpolation=cv2.INTER_CUBIC)\n","    depth = cv2.resize(depth[0, 0].cpu().data.numpy(),\n","                    img.shape[:2][::-1],\n","                    interpolation=cv2.INTER_CUBIC)\n","    segm = CMAP[segm.argmax(axis=2)].astype(np.uint8)\n","    depth = np.abs(depth)\n","    return depth, segm"],"metadata":{"id":"k4jSjgJQU1f0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["depth, segm = pipeline(img)"],"metadata":{"id":"Id8WbXlQLUm8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,20))\n","ax1.imshow(img)\n","ax1.set_title('Original', fontsize=30)\n","ax2.imshow(segm)\n","ax2.set_title('Predicted Segmentation', fontsize=30)\n","ax3.imshow(depth, cmap=\"plasma\", vmin=0, vmax=80)\n","ax3.set_title(\"Predicted Depth\", fontsize=30)\n","plt.show()"],"metadata":{"id":"A6M_DJ4rPG1m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.4 — Run on a Video"],"metadata":{"id":"-9309NqKKv0D"}},{"cell_type":"code","source":["print(img.shape)\n","print(depth.shape)\n","print(segm.shape)"],"metadata":{"id":"5Kev1r-xMGwU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.cm as cm\n","import matplotlib.colors as co\n","\n","def depth_to_rgb(depth):\n","    normalizer = co.Normalize(vmin=0, vmax=80)\n","    mapper = cm.ScalarMappable(norm=normalizer, cmap='plasma')\n","    colormapped_im = (mapper.to_rgba(depth)[:, :, :3] * 255).astype(np.uint8)\n","    return colormapped_im\n","\n","depth_rgb = depth_to_rgb(depth)\n","print(depth_rgb.shape)\n","plt.imshow(depth_rgb)\n","plt.show()"],"metadata":{"id":"6OREeqr7WRJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(img.shape)\n","print(depth_rgb.shape)\n","print(segm.shape)\n","new_img = np.vstack((img, segm, depth_rgb))\n","plt.imshow(new_img)\n","plt.show()"],"metadata":{"id":"xP2KD52CMnKh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["video_files = sorted(glob.glob(\"data/*.png\"))\n","\n","# Build a HydraNet\n","hydranet = HydraNet()\n","hydranet.define_mobilenet()\n","hydranet.define_lightweight_refinenet()\n","hydranet._initialize_weights()\n","\n","# Set the Model to Eval on GPU\n","if torch.cuda.is_available():\n","    _ = hydranet.cuda()\n","_ = hydranet.eval()\n","\n","# Load the Weights\n","ckpt = torch.load('ExpKITTI_joint.ckpt')\n","hydranet.load_state_dict(ckpt['state_dict'])\n","\n","# Run the pipeline\n","result_video = []\n","for idx, img_path in enumerate(video_files):\n","    image = np.array(Image.open(img_path))\n","    h, w, _ = image.shape \n","    depth, seg = pipeline(image)\n","    result_video.append(cv2.cvtColor(cv2.vconcat([image, seg, depth_to_rgb(depth)]), cv2.COLOR_BGR2RGB))\n","\n","out = cv2.VideoWriter('output/out.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 15, (w,3*h))\n","\n","for i in range(len(result_video)):\n","    out.write(result_video[i])\n","out.release()"],"metadata":{"id":"l3u7PgoqKuyV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import HTML\n","from base64 import b64encode\n","mp4 = open('output/out.mp4','rb').read()\n","data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","HTML(\"\"\"\n","<video width=800 controls>\n","      <source src=\"%s\" type=\"video/mp4\">\n","</video>\n","\"\"\" % data_url)"],"metadata":{"id":"15-z-lNVxaRX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3D Segmentation\n","\n","Did you ever wonder... How is segmentation used in self-driving cars? Like, **once you have the map, what do you do with it**?\n","<p>\n","Let's see something called 3D Segmentation — Fusing a Depth Map with a Segmentation Map!\n","<p>\n","\n","In my course [MASTER STEREO VISION](https://courses.thinkautonomous.ai/stereo-vision), I teach how to do something called **3D Reconstruction** from a Depth Map and Calibration Parameters. <p>\n","In this course, we're going to see how to do it with Open3D, my go-to library for Point Clouds, and we'll see how to build 3D Segmentation Algorithms by fusing the Depth Map (3D) with the Segmentation Map."],"metadata":{"id":"dcSVxVHqp2g6"}},{"cell_type":"code","source":["!pip install open3d==0.14.1"],"metadata":{"id":"YGmsHUMgsxHT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import open3d as o3d"],"metadata":{"id":"gl4NqJC7sus2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["o3d.__version__"],"metadata":{"id":"BZUSRdrxwPHx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### RGBD - Fuse the RGB Image and the Depth Map\n","\n","The first thing we'll implement is to create an RGBD Image by fusing the RGB Image with the Depth Map. For that, we'll use [Open3D's Class RGBD Image](http://www.open3d.org/docs/release/python_api/open3d.geometry.RGBDImage.html) and the function create_from_color_and_depth(color, depth).<p>\n","It looks pretty straghtforward, we just need to make sure that the image are loaded as [Open3D Images](http://www.open3d.org/docs/release/python_api/open3d.geometry.Geometry.html?highlight=image#open3d.geometry.Geometry.Image)."],"metadata":{"id":"a0MwLXd-1Lce"}},{"cell_type":"code","source":["rgbd = #TODO: Call the Function"],"metadata":{"id":"CduFT43RUaxH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we'll use the function create_from_rgbd_image to build a Point Cloud based on this. For that, we'll need the camera's intrinsic parameters. <p>\n","If you'd like to learn more about this, I invite you to take my course on [Stereo Vision](https://courses.thinkautonomous.ai/stereo-vision). In this course, I'm just going to give'em to you."],"metadata":{"id":"osFtnlbzUbp-"}},{"cell_type":"code","source":["o3d.camera.PinholeCameraIntrinsic??"],"metadata":{"id":"3n9Wuq3fvr5y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["intrinsics = o3d.camera.PinholeCameraIntrinsic(width = 1242, height = 375, fx = 721., fy = 721., cx = 609., cy = 609.)"],"metadata":{"id":"NZWxv0-Htine"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["point_cloud = #TODO: Create A Point Cloud\n","o3d.io.write_point_cloud(\"test.pcd\", point_cloud)"],"metadata":{"id":"m6rpNbLSTon4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3D Segmentation — Fuse the Segmentation Map with the Depth Map\n","From now on, the process is exactly the same. But instead of creating a Point Cloud from an RGBD Image with the Normal RGB Image, we'll do it with the Depth Map."],"metadata":{"id":"dXkvW7hA1Nzl"}},{"cell_type":"code","source":["rgbd = #TODO: Call the Function"],"metadata":{"id":"8Vt6VWphzsDz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["point_cloud = #TODO: Create A Point Cloud"],"metadata":{"id":"HDw31-VR0HwC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["o3d.io.write_point_cloud(\"test_segm.pcd\", point_cloud)"],"metadata":{"id":"IR_UrU-i0wLV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UleBi3tUeAST"},"execution_count":null,"outputs":[]}]}