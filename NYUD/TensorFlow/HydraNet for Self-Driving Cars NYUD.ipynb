{"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"gpuClass":"standard"},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Welcome to the HydraNet Home Robot Workshop üê∏üê∏üê∏\n","\n","In this workshop, you're going to learn how to train a Neural Network that does **real-time semantic segmentation and monocular depth prediction**.\n","\n","![](https://d3i71xaburhd42.cloudfront.net/435d4b5c30f10753d277848a17baddebd98d3c31/2-Figure1-1.png)\n","\n","The Model is [a Multi-Task Learning algorithm designed by Vladimir Nekrasov](https://arxiv.org/pdf/1809.04766.pdf). The entire work is based on the **DenseTorch Library**, that you can find and use [here](https://github.com/DrSleep/DenseTorch). <p>\n","\n","**A note ‚Äî** This notebook is adapting the Library with express authorization from the author for educational purpose.\n","\n","## Home Robot ü§ñ\n","* In the previous workshop of the course, you learned how to design the model shown above, and to run it on the KITTI Dataset using pretrained weights. The **KITTI Dataset only has 200 examples of segmentation**. Therefore, the authors used a technique called Knowledge Distillation and finetuned using the Cityscape dataset.<p>\n","\n","* üëâ In our case, we'll use another dataset called the [NYUDv2 Dataset](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html). **It contains 1449 annotated images for depth and segmentation**, which makes our life much simpler. ‚Äî‚Äî Since this is an indoor dataset, we'll turn this project into a Home Robot Workshop!"],"metadata":{"id":"20Fk1u9m9xfl"}},{"cell_type":"markdown","source":["#1 ‚Äî¬†Imports\n","\n","We're going to import:\n","*   The **Data from our previous notebook** (trained model, cmaps, ...)\n","*   The **NYUD Dataset**, along with helper files, ground truth examples, and train/test split files\n","\n","\n"],"metadata":{"id":"5Ix2A28T-ZT_"}},{"cell_type":"code","source":["# Install the tensorflow-addons and onnx-tensorflow and update tensorflow\n","!pip install -U tensorflow\n","!git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow && pip install -e .\n","!pip install tensorflow-addons"],"metadata":{"id":"P1U2fqgcv7t5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://hydranets-data.s3.eu-west-3.amazonaws.com/hydranets-data-2.zip && unzip -q hydranets-data-2.zip && mv hydranets-data-2/* . && rm hydranets-data-2.zip && rm -rf hydranets-data-2"],"metadata":{"id":"f5QS8L_xCBLo","execution":{"iopub.status.busy":"2022-12-21T03:11:50.615649Z","iopub.execute_input":"2022-12-21T03:11:50.616003Z","iopub.status.idle":"2022-12-21T03:12:14.018353Z","shell.execute_reply.started":"2022-12-21T03:11:50.615976Z","shell.execute_reply":"2022-12-21T03:12:14.017579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"oZj4cXmXACJP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%matplotlib inline\n","import sys\n","sys.path.append(\"./onnx-tensorflow\")\n","import os\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","# import tensorflow.compat.v1 as tf\n","# tf.disable_v2_behavior()\n","import tensorflow_addons as tfa\n","import glob\n","import onnx\n","from onnx_tf.backend import prepare"],"metadata":{"id":"szGHb1YQSyJ_","execution":{"iopub.status.busy":"2022-12-21T03:20:19.088050Z","iopub.execute_input":"2022-12-21T03:20:19.088480Z","iopub.status.idle":"2022-12-21T03:20:19.095854Z","shell.execute_reply.started":"2022-12-21T03:20:19.088447Z","shell.execute_reply":"2022-12-21T03:20:19.095126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1 ‚Äî Dataset\n","Let's begin with importing our data, and visualizing it."],"metadata":{"id":"8snxsl9eACAs"}},{"cell_type":"markdown","source":["## Load and Visualize the Dataset"],"metadata":{"id":"FsAvSppK_VLY"}},{"cell_type":"code","source":["depth = sorted(glob.glob('/content/nyud/depth/*.png'))\n","seg = sorted(glob.glob('/content/nyud/masks/*.png'))\n","images = sorted(glob.glob('/content/nyud/rgb/*.png'))"],"metadata":{"id":"UjQLkkLRULyU","execution":{"iopub.status.busy":"2022-12-21T03:20:19.617992Z","iopub.execute_input":"2022-12-21T03:20:19.618539Z","iopub.status.idle":"2022-12-21T03:20:19.635744Z","shell.execute_reply.started":"2022-12-21T03:20:19.618493Z","shell.execute_reply":"2022-12-21T03:20:19.634823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(images))\n","print(len(depth))\n","print(len(seg))"],"metadata":{"id":"f3kIx0CPHeJ_","execution":{"iopub.status.busy":"2022-12-21T03:20:19.829708Z","iopub.execute_input":"2022-12-21T03:20:19.830067Z","iopub.status.idle":"2022-12-21T03:20:19.835089Z","shell.execute_reply.started":"2022-12-21T03:20:19.830040Z","shell.execute_reply":"2022-12-21T03:20:19.834078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since our dataset is a bit \"special\", we'll need a Color Map to read it."],"metadata":{"id":"XoZp3xq8jjYI"}},{"cell_type":"code","source":["CMAP = np.load('cmap_nyud.npy')\n","print(len(CMAP))"],"metadata":{"id":"qIhWdE4Hd_Ul","execution":{"iopub.status.busy":"2022-12-21T03:20:20.253102Z","iopub.execute_input":"2022-12-21T03:20:20.254207Z","iopub.status.idle":"2022-12-21T03:20:20.258713Z","shell.execute_reply.started":"2022-12-21T03:20:20.254177Z","shell.execute_reply":"2022-12-21T03:20:20.258115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = np.random.randint(0,len(seg))\n","\n","f, (ax0, ax1, ax2) = plt.subplots(1,3, figsize=(20,40))\n","ax0.imshow(np.array(Image.open(images[idx])))\n","ax0.set_title(\"Original\")\n","ax1.imshow(np.array(Image.open(depth[idx])), cmap=\"plasma\")\n","ax1.set_title(\"Depth\")\n","ax2.imshow(CMAP[np.array(Image.open(seg[idx]))])\n","ax2.set_title(\"Segmentation\")\n","plt.show()"],"metadata":{"id":"6VzwxQ42D97D","execution":{"iopub.status.busy":"2022-12-21T03:20:20.481617Z","iopub.execute_input":"2022-12-21T03:20:20.482695Z","iopub.status.idle":"2022-12-21T03:20:21.079230Z","shell.execute_reply.started":"2022-12-21T03:20:20.482666Z","shell.execute_reply":"2022-12-21T03:20:21.078462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(np.unique(np.array(Image.open(seg[idx]))))\n","print(len(np.unique(np.array(Image.open(seg[idx])))))"],"metadata":{"id":"7RxtjtR_eRgt","execution":{"iopub.status.busy":"2022-12-21T03:20:21.080636Z","iopub.execute_input":"2022-12-21T03:20:21.081687Z","iopub.status.idle":"2022-12-21T03:20:21.100912Z","shell.execute_reply.started":"2022-12-21T03:20:21.081657Z","shell.execute_reply":"2022-12-21T03:20:21.099912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Getting the DataLoader\n","\n","When training a model, 2 elements are going to be very important (compared to the last workshop):\n","\n","*   The Dataset\n","*   The Training Loop, Loss, etc\n","\n","We already know how to design the model that does join depth and segmentation, so we only need to know how to train it!"],"metadata":{"id":"LrrBK7QY_Y_z"}},{"cell_type":"code","source":["# Create the train dataset\n","train_data_file = \"train_list_depth.txt\"\n","\n","with open(train_data_file, \"rb\") as f:\n","    train_datalist = f.readlines()\n","train_datalist = [x.decode(\"utf-8\").strip(\"\\n\").split(\"\\t\") for x in train_datalist]\n","\n","root_dir = \"/content/nyud\"\n","masks_names = (\"segm\", \"depth\")\n","\n","print(train_datalist[0])"],"metadata":{"id":"SrAXPd5j63Cw","execution":{"iopub.status.busy":"2022-12-21T03:20:21.287638Z","iopub.execute_input":"2022-12-21T03:20:21.288474Z","iopub.status.idle":"2022-12-21T03:20:21.296160Z","shell.execute_reply.started":"2022-12-21T03:20:21.288445Z","shell.execute_reply":"2022-12-21T03:20:21.295170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the valid dataset\n","valid_data_file = \"val_list_depth.txt\"\n","\n","with open(valid_data_file, \"rb\") as f:\n","    valid_datalist = f.readlines()\n","valid_datalist = [x.decode(\"utf-8\").strip(\"\\n\").split(\"\\t\") for x in valid_datalist]\n","\n","print(valid_datalist[0])"],"metadata":{"id":"f5lOZj380sp6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize lists for train rgb, seg and depth and valid rgb, seg and depth\n","train_rgb = []\n","train_seg = []\n","train_depth = []\n","\n","valid_rgb = []\n","valid_seg = []\n","valid_depth = []\n","\n","# Loop over the train_datalist and append the filenames to the appropriate lists\n","for i in train_datalist :\n","  train_rgb.append(os.path.join(root_dir, i[0]))\n","  train_seg.append(os.path.join(root_dir, i[1]))\n","  train_depth.append(os.path.join(root_dir, i[2]))\n","\n","# Loop over the valid_datalist and append the filenames to the appropriate lists\n","for i in valid_datalist :\n","  valid_rgb.append(os.path.join(root_dir, i[0]))\n","  valid_seg.append(os.path.join(root_dir, i[1]))\n","  valid_depth.append(os.path.join(root_dir, i[2]))"],"metadata":{"id":"K8ewJNJa1Eny"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the train and valid dataframes for the dataloaders\n","train_df = pd.DataFrame(data={'rgb':train_rgb, 'seg':train_seg, 'depth':train_depth})\n","valid_df = pd.DataFrame(data={'rgb':valid_rgb, 'seg':valid_seg, 'depth':valid_depth})\n","train_df.shape, valid_df.shape"],"metadata":{"id":"3-pXFrUj2D6K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_arr = np.array(Image.open(train_df['rgb'][0]))\n","\n","plt.imshow(img_arr)\n","plt.show()"],"metadata":{"id":"Ix5sznp37XdA","execution":{"iopub.status.busy":"2022-12-21T03:20:21.695391Z","iopub.execute_input":"2022-12-21T03:20:21.695992Z","iopub.status.idle":"2022-12-21T03:20:21.895096Z","shell.execute_reply.started":"2022-12-21T03:20:21.695949Z","shell.execute_reply":"2022-12-21T03:20:21.894319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["masks_names = (\"segm\", \"depth\")\n","\n","for mask_name, mask_path in zip(masks_names, train_df.iloc[0][1:]):\n","    print(mask_name)\n","    print(mask_path)\n","    mask = np.array(Image.open(mask_path))\n","    plt.imshow(mask)\n","    plt.show()"],"metadata":{"id":"TfjiDlLr9yAs","execution":{"iopub.status.busy":"2022-12-21T03:20:21.896777Z","iopub.execute_input":"2022-12-21T03:20:21.897277Z","iopub.status.idle":"2022-12-21T03:20:22.199616Z","shell.execute_reply.started":"2022-12-21T03:20:21.897247Z","shell.execute_reply":"2022-12-21T03:20:22.198680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Normalization ‚Äî Will be common to all images\n"],"metadata":{"id":"IzR6cnx5-6oT"}},{"cell_type":"code","source":["def preprocess_image(img) :\n","    mean = [0.485, 0.456, 0.406] # ImageNet\n","    std = [0.229, 0.224, 0.225] # ImageNet\n","    img = (img - mean) / std\n","    img = tf.image.random_crop(img, size=[400,400])\n","    return img\n","  "],"metadata":{"id":"eW0FmJbD5D1m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from utils import Normalise, RandomCrop, ToTensor, RandomMirror\n","# import torchvision.transforms as transforms"],"metadata":{"id":"oUOgFLww2rxt","execution":{"iopub.status.busy":"2022-12-21T03:12:16.643178Z","iopub.execute_input":"2022-12-21T03:12:16.643686Z","iopub.status.idle":"2022-12-21T03:12:17.071960Z","shell.execute_reply.started":"2022-12-21T03:12:16.643659Z","shell.execute_reply":"2022-12-21T03:12:17.070972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# img_scale = 1.0 / 255\n","# depth_scale = 5000.0\n","\n","# img_mean = np.array([0.485, 0.456, 0.406])\n","# img_std = np.array([0.229, 0.224, 0.225])\n","\n","# normalise_params = [img_scale, img_mean.reshape((1, 1, 3)), img_std.reshape((1, 1, 3)), depth_scale,]\n","\n","# transform_common = [Normalise(*normalise_params), ToTensor()]"],"metadata":{"id":"NuKBfbOb-5ll","execution":{"iopub.status.busy":"2022-12-21T03:12:17.073508Z","iopub.execute_input":"2022-12-21T03:12:17.073902Z","iopub.status.idle":"2022-12-21T03:12:17.080945Z","shell.execute_reply.started":"2022-12-21T03:12:17.073868Z","shell.execute_reply":"2022-12-21T03:12:17.079865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Transforms"],"metadata":{"id":"_7B5LPe1_et-"}},{"cell_type":"code","source":["# crop_size = 400\n","# transform_train = transforms.Compose([RandomMirror(), RandomCrop(crop_size)] + transform_common)\n","# transform_val = transforms.Compose(transform_common)"],"metadata":{"id":"UdwYse7G_tip","execution":{"iopub.status.busy":"2022-12-21T03:12:17.082283Z","iopub.execute_input":"2022-12-21T03:12:17.082659Z","iopub.status.idle":"2022-12-21T03:12:17.090479Z","shell.execute_reply.started":"2022-12-21T03:12:17.082628Z","shell.execute_reply":"2022-12-21T03:12:17.089651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### DataLoader"],"metadata":{"id":"7NvjcDfO_tjr"}},{"cell_type":"code","source":["BATCH_SIZE = 4\n","\n","# Create a generator to create the train and validation dataloaders\n","generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255., preprocessing_function=preprocess_image,horizontal_flip=True,vertical_flip=True)\n","\n","train_dataloader = generator.flow_from_dataframe(train_df, directory=None, x_col='rgb', y_col=['seg', 'depth'], class_mode='multi_output',\n","                                                 color_model='rgb', target_size=(32,32), batch_size=BATCH_SIZE, shuffle=True)\n","\n","valid_dataloader = generator.flow_from_dataframe(valid_df, directory=None, x_col='rgb', y_col=['seg', 'depth'], class_mode='multi_output',\n","                                                 color_model='rgb', target_size=(32,32), batch_size=BATCH_SIZE, shuffle=False)\n","\n","train_steps = len(train_dataloader)//BATCH_SIZE\n","valid_steps = len(valid_dataloader)//BATCH_SIZE"],"metadata":{"id":"N34rQM7J8GVo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_batch_size = 4\n","# val_batch_size = 4\n","# train_file = \"train_list_depth.txt\"\n","# val_file = \"val_list_depth.txt\""],"metadata":{"id":"0d82jskZ_vZo","execution":{"iopub.status.busy":"2022-12-21T03:12:17.091708Z","iopub.execute_input":"2022-12-21T03:12:17.091967Z","iopub.status.idle":"2022-12-21T03:12:17.099531Z","shell.execute_reply.started":"2022-12-21T03:12:17.091944Z","shell.execute_reply":"2022-12-21T03:12:17.098641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from torch.utils.data import DataLoader\n","\n","# #TRAIN DATALOADER\n","# trainloader = DataLoader(\n","#     HydranetDataset(train_file, transform=transform_train,),\n","#     batch_size=train_batch_size,\n","#     shuffle=True,\n","#     num_workers=4,\n","#     pin_memory=True,\n","#     drop_last=True)\n","\n","# # VALIDATION DATALOADER\n","# valloader = DataLoader(HydranetDataset(val_file, transform=transform_val,),\n","#     batch_size=val_batch_size, \n","#     shuffle=False, num_workers=4, \n","#     pin_memory=True,\n","#     drop_last=False)"],"metadata":{"id":"kQogy3tk03TP","execution":{"iopub.status.busy":"2022-12-21T03:12:17.100839Z","iopub.execute_input":"2022-12-21T03:12:17.101071Z","iopub.status.idle":"2022-12-21T03:12:17.110380Z","shell.execute_reply.started":"2022-12-21T03:12:17.101049Z","shell.execute_reply":"2022-12-21T03:12:17.109647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2 ‚Äî Creating the HydraNet\n","We now have 2 DataLoaders: one for training, and one for validation/test. <p>\n","\n","In the next step, we're going to define our model, following the paper [Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations](https://arxiv.org/pdf/1809.04766.pdf) ‚Äî‚Äî If you haven't read it yet, now is the time.\n","<p>\n","\n","> ![](https://d3i71xaburhd42.cloudfront.net/435d4b5c30f10753d277848a17baddebd98d3c31/2-Figure1-1.png)\n","\n","Our model takes an input RGB image, make it go through an encoder, a lightweight refinenet decoder, and then has 2 heads, one for each task.<p>\n","Things to note:\n","* The only **convolutions** we'll need will be 3x3 and 1x1\n","* We also need a **MaxPooling 5x5**\n","* **CRP-Blocks** are implemented as Skip-Connection Operations\n","* **Each Head is made of a 1x1 convolution followed by a 3x3 convolution**, only the data and the loss change there\n"],"metadata":{"id":"LuR-bk-13cX6"}},{"cell_type":"markdown","source":["## Building the Encoder ‚Äî A MobileNetv2\n","![](https://iq.opengenus.org/content/images/2020/11/conv_mobilenet_v2.jpg)"],"metadata":{"id":"025K4utrBqNy"}},{"cell_type":"code","source":["def conv3x3(filters, stride=1, dilation=1, groups=1, bias=False):\n","    # 3x3 convolution\n","    return tf.keras.layers.Conv2D(filters, kernel_size=3, strides=stride,\n","                     padding='same', dilation_rate=dilation, use_bias=bias, groups=groups)"],"metadata":{"id":"TpCvdaopBGqo","execution":{"iopub.status.busy":"2022-12-21T03:12:17.111739Z","iopub.execute_input":"2022-12-21T03:12:17.112013Z","iopub.status.idle":"2022-12-21T03:12:17.122561Z","shell.execute_reply.started":"2022-12-21T03:12:17.111940Z","shell.execute_reply":"2022-12-21T03:12:17.121458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def conv1x1(filters, stride=1, groups=1, bias=False, ):\n","    # 1x1 convolution\n","    return tf.keras.layers.Conv2D(filters, kernel_size=1, strides=stride,\n","                     padding='valid', use_bias=bias, groups=groups)"],"metadata":{"id":"X26XhiFLBZD5","execution":{"iopub.status.busy":"2022-12-21T03:12:17.123681Z","iopub.execute_input":"2022-12-21T03:12:17.124159Z","iopub.status.idle":"2022-12-21T03:12:17.131508Z","shell.execute_reply.started":"2022-12-21T03:12:17.124135Z","shell.execute_reply":"2022-12-21T03:12:17.130691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def batchnorm():\n","    # batch norm 2d\n","    batch_norm = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1)\n","    batch_norm.trainable = True\n","    return batch_norm"],"metadata":{"id":"inBcLWf3jGvd","execution":{"iopub.status.busy":"2022-12-21T03:12:17.132422Z","iopub.execute_input":"2022-12-21T03:12:17.132818Z","iopub.status.idle":"2022-12-21T03:12:17.140945Z","shell.execute_reply.started":"2022-12-21T03:12:17.132790Z","shell.execute_reply":"2022-12-21T03:12:17.139907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convbnrelu(filters, kernel_size, stride=1, groups=1, act=True):\n","    # conv-batchnorm-relu\n","    if int(kernel_size/2) == 1 :\n","        padding = 'same'\n","    if int(kernel_size/2) == 0 :\n","        padding = 'valid'\n","    if act:\n","        return tf.keras.Sequential([tf.keras.layers.Conv2D(filters, kernel_size, strides=stride, padding=padding, groups=groups, use_bias=False),\n","                             batchnorm(),\n","                             tf.keras.layers.ReLU(max_value=6)])\n","    else:\n","        return tf.keras.Sequential([tf.keras.layers.Conv2D(filters, kernel_size, strides=stride, padding=padding, groups=groups, use_bias=False),\n","                             batchnorm()])"],"metadata":{"id":"vDJDvGiQSy2L","execution":{"iopub.status.busy":"2022-12-21T03:12:17.142473Z","iopub.execute_input":"2022-12-21T03:12:17.142828Z","iopub.status.idle":"2022-12-21T03:12:17.150613Z","shell.execute_reply.started":"2022-12-21T03:12:17.142788Z","shell.execute_reply":"2022-12-21T03:12:17.149538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class InvertedResidualBlock(tf.keras.Model) :\n","  def __init__(self,in_planes, filters, expansion_factor, stride) :\n","    super(InvertedResidualBlock, self).__init__()\n","    intermed_planes = in_planes * expansion_factor\n","    self.residual = (in_planes == filters) and (stride == 1) # Boolean/Condition\n","    self.IBR = tf.keras.Sequential([convbnrelu(in_planes, kernel_size=1, stride=stride, act=True), \n","                            convbnrelu(intermed_planes, kernel_size=3, \n","                                      stride=stride, groups=intermed_planes, act=True), \n","                            convbnrelu(filters, kernel_size=1, stride=stride, act=False)])\n","        \n","  def call(self, inputs) :\n","    x = self.IBR(inputs)\n","    if self.residual :\n","        return (x + inputs)\n","    else :\n","        return x"],"metadata":{"id":"gEXj3l0OBi1w","execution":{"iopub.status.busy":"2022-12-21T03:12:17.152350Z","iopub.execute_input":"2022-12-21T03:12:17.153285Z","iopub.status.idle":"2022-12-21T03:12:17.160435Z","shell.execute_reply.started":"2022-12-21T03:12:17.153253Z","shell.execute_reply":"2022-12-21T03:12:17.159559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MobileNetV2 (tf.keras.Model) :\n","    def __init__(self):\n","        super(MobileNetV2, self).__init__()\n","        self.LAYERS=[]\n","        mobilenet_config = [[1, 16, 1, 1], # expansion rate, output channels, number of repeats, stride\n","                        [6, 24, 2, 2],\n","                        [6, 32, 3, 2],\n","                        [6, 64, 4, 2],\n","                        [6, 96, 3, 1],\n","                        [6, 160, 3, 2],\n","                        [6, 320, 1, 1],\n","                        ]\n","        self.in_channels = 32 # number of input channels\n","        self.num_layers = len(mobilenet_config)\n","        self.layer1 = convbnrelu(filters=32, kernel_size=3, stride=2) # This is the first layer of the first \n","        \n","        self.layer1_model = tf.keras.Sequential(self.layer1)\n","        # No need to name the model as we will have a call function for this class\n","        # self.layer1_model._name = 'layer1'\n","        \n","        self.LAYERS.append(self.layer1_model)\n","        \n","        # No need to initialize this sequential as well!\n","        # encoder = tf.keras.Sequential()\n","        # encoder.add(layer1_model)\n","        \n","        \n","        c_layer = 2\n","        for t,c,n,s in (mobilenet_config):\n","            layers = []\n","            for idx in range(n):\n","                layers.append(InvertedResidualBlock(self.in_channels, c, expansion_factor=t, stride=s if idx == 0 else 1))\n","                self.in_channels = c\n","                \n","            # NO need for this as well\n","            # model = tf.keras.Sequential(layers)\n","            # model._name = f'layer{c_layer}'\n","            # print(model._name)\n","            # encoder.add(model)\n","            \n","            # Add the model to the LAYERS\n","            self.LAYERS.append(tf.keras.Sequential(layers))\n","            c_layer += 1\n","            \n","        self.layer1, self.layer2, self.layer3, self.layer4, self.layer5, self.layer6, self.layer7, self.layer8 = self.LAYERS\n","        \n","        \n","    def call(self, inputs) :\n","        l1 = self.layer1(inputs) # SELF.LAYERS[0](inputs)\n","        l2 = self.layer2(l1) # SELF.LAYERS[1](l1)\n","        l3 = self.layer3(l2) # SELF.LAYERS[2](l2)\n","        l4 = self.layer4(l3) # SELF.LAYERS[3](l3)\n","        l5 = self.layer5(l4) # SELF.LAYERS[4](l4)\n","        l6 = self.layer6(l5) # SELF.LAYERS[5](l5)\n","        l7 = self.layer7(l6) # SELF.LAYERS[6](l6)\n","        l8 = self.layer8(l7) # SELF.LAYERS[7](l7)\n","        \n","        return l3, l4, l5, l6, l7, l8"],"metadata":{"id":"Hdl3llB-85fH","execution":{"iopub.status.busy":"2022-12-21T03:12:17.162023Z","iopub.execute_input":"2022-12-21T03:12:17.162680Z","iopub.status.idle":"2022-12-21T03:12:17.176943Z","shell.execute_reply.started":"2022-12-21T03:12:17.162645Z","shell.execute_reply":"2022-12-21T03:12:17.175892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class MobileNetv2(tf.keras.Model):\n","#     def __init__(self, return_idx=[6]):\n","#         super(MobileNetv2, self).__init__()\n","#         # expansion rate, output channels, number of repeats, stride\n","#         self.mobilenet_config = [\n","#         [1, 16, 1, 1],\n","#         [6, 24, 2, 2],\n","#         [6, 32, 3, 2],\n","#         [6, 64, 4, 2],\n","#         [6, 96, 3, 1],\n","#         [6, 160, 3, 2],\n","#         [6, 320, 1, 1],\n","#         ]\n","#         self.in_channels = 32  # number of input channels\n","#         self.num_layers = len(self.mobilenet_config)\n","#         self.layer1 = convbnrelu(3, self.in_channels, kernel_size=3, stride=2)\n","    \n","#         self.return_idx = [1, 2, 3, 4, 5, 6]\n","#         #self.return_idx = make_list(return_idx)\n","\n","#         c_layer = 2\n","#         for t, c, n, s in self.mobilenet_config:\n","#             layers = []\n","#             for idx in range(n):\n","#                 layers.append(InvertedResidualBlock(self.in_channels,c,expansion_factor=t,stride=s if idx == 0 else 1,))\n","#                 self.in_channels = c\n","#             setattr(self, \"layer{}\".format(c_layer), nn.Sequential(*layers))\n","#             c_layer += 1\n","\n","#         self._out_c = [self.mobilenet_config[idx][1] for idx in self.return_idx] # Output: [24, 32, 64, 96, 160, 320]\n","\n","#     def call(self, x):\n","#         outs = []\n","#         x = self.layer1(x)\n","#         outs.append(self.layer2(x))  # 16, x / 2\n","#         outs.append(self.layer3(outs[-1]))  # 24, x / 4\n","#         outs.append(self.layer4(outs[-1]))  # 32, x / 8\n","#         outs.append(self.layer5(outs[-1]))  # 64, x / 16\n","#         outs.append(self.layer6(outs[-1]))  # 96, x / 16\n","#         outs.append(self.layer7(outs[-1]))  # 160, x / 32\n","#         outs.append(self.layer8(outs[-1]))  # 320, x / 32\n","\n","#         return [outs[idx] for idx in self.return_idx]"],"metadata":{"id":"s2uBocHRCrQF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# encoder = MobileNetV2()\n","# encoder.load_state_dict(torch.load(\"mobilenetv2-e6e8dd43.pth\"))\n","# onnx_model = onnx.load('/content/drive/MyDrive/Colab Notebooks/HydraNets/NYUD/TensorFlow/mobilenet-v2.onnx')\n","# encoder = prepare(onnx_model)\n","# encoder.export_graph(\"model.pb\")"],"metadata":{"id":"YRmoJbmpkR0l","execution":{"iopub.status.busy":"2022-12-21T03:12:17.177955Z","iopub.execute_input":"2022-12-21T03:12:17.178203Z","iopub.status.idle":"2022-12-21T03:12:17.253764Z","shell.execute_reply.started":"2022-12-21T03:12:17.178179Z","shell.execute_reply":"2022-12-21T03:12:17.252833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(encoder)"],"metadata":{"id":"d7bg7yzbm5Cz","execution":{"iopub.status.busy":"2022-12-21T02:04:44.947988Z","iopub.execute_input":"2022-12-21T02:04:44.948610Z","iopub.status.idle":"2022-12-21T02:04:44.953238Z","shell.execute_reply.started":"2022-12-21T02:04:44.948582Z","shell.execute_reply":"2022-12-21T02:04:44.952088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Building the Decoder - A Multi-Task Lighweight RefineNet\n","Paper: https://arxiv.org/pdf/1810.03272.pdf\n","![](https://drsleep.github.io/images/rf_arch.png)"],"metadata":{"id":"ymQKYmnjCEng"}},{"cell_type":"code","source":["def make_list(x):\n","    \"\"\"Returns the given input as a list.\"\"\"\n","    if isinstance(x, list):\n","        return x\n","    elif isinstance(x, tuple):\n","        return list(x)\n","    else:\n","        return [x]"],"metadata":{"id":"8Fneuzuu_7Et","execution":{"iopub.status.busy":"2022-12-21T02:04:49.741487Z","iopub.execute_input":"2022-12-21T02:04:49.741918Z","iopub.status.idle":"2022-12-21T02:04:49.747143Z","shell.execute_reply.started":"2022-12-21T02:04:49.741884Z","shell.execute_reply":"2022-12-21T02:04:49.746108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CRPBlock(tf.keras.Model):\n","    \"\"\"CRP definition\"\"\"\n","    def __init__(self, in_planes, out_planes, n_stages, groups=False):\n","        super().__init__()\n","        for i in range(n_stages):\n","            setattr(self, '{}_{}'.format(i + 1, 'outvar_dimred'),\n","                    conv1x1(in_planes if (i == 0) else out_planes,\n","                            out_planes, stride=1,\n","                            bias=False, groups=in_planes if groups else 1))\n","        self.stride = 1\n","        self.n_stages = n_stages\n","        self.maxpool = tf.keras.layers.MaxPool2D(pool_size=5, strides=1, padding='same')\n","\n","    def forward(self, x):\n","        top = x\n","        for i in range(self.n_stages):\n","            top = self.maxpool(top)\n","            top = getattr(self, '{}_{}'.format(i + 1, 'outvar_dimred'))(top)\n","            x = top + x\n","        return x"],"metadata":{"id":"N6Ia8Cm2BhN6","execution":{"iopub.status.busy":"2022-12-21T02:04:51.877511Z","iopub.execute_input":"2022-12-21T02:04:51.878213Z","iopub.status.idle":"2022-12-21T02:04:51.886274Z","shell.execute_reply.started":"2022-12-21T02:04:51.878178Z","shell.execute_reply":"2022-12-21T02:04:51.884970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MTLWRefineNet(tf.keras.Model):\n","    def __init__(self, input_sizes, num_classes, agg_size=256, n_crp=4):\n","        super(MTLWRefineNet, self).__init__()\n","\n","        stem_convs = list()\n","        crp_blocks = list()\n","        adapt_convs = list()\n","        heads = list()\n","\n","        # Reverse since we recover information from the end\n","        input_sizes = list(reversed((input_sizes)))\n","\n","        # No reverse for collapse indices is needed\n","        self.collapse_ind = [[0, 1], [2, 3], 4, 5]\n","\n","        groups = [False] * len(self.collapse_ind)\n","        groups[-1] = True\n","\n","        for size in input_sizes:\n","            stem_convs.append(conv1x1(size, agg_size, bias=False))\n","\n","        for group in groups:\n","            crp_blocks.append(self._make_crp(agg_size, agg_size, n_crp, group))\n","            adapt_convs.append(conv1x1(agg_size, agg_size, bias=False))\n","\n","        self.stem_convs = stem_convs\n","        self.crp_blocks = crp_blocks\n","        self.adapt_convs = adapt_convs[:-1]\n","\n","        num_classes = list(num_classes)\n","        for n_out in num_classes:\n","            heads.append(\n","                tf.keras.Sequential(\n","                    conv1x1(agg_size, agg_size, groups=agg_size, bias=False),\n","                    tf.keras.layers.ReLU(6),\n","                    conv3x3(agg_size, n_out, bias=True),\n","                )\n","            )\n","\n","        self.heads = heads\n","        self.relu = tf.keras.layers.ReLU(6)\n","\n","    @staticmethod\n","    def _make_crp(in_planes, out_planes, stages, groups):\n","        # Same as previous, but showing the use of a @staticmethod\n","        layers = [CRPBlock(in_planes, out_planes, stages, groups)]\n","        return tf.keras.Sequential(layers)\n","\n","    def call(self, xs):\n","        xs = list(reversed(xs))\n","        for idx, (conv, x) in enumerate(zip(self.stem_convs, xs)):\n","            xs[idx] = conv(x)\n","\n","        # Collapse layers\n","        c_xs = [sum([xs[idx] for idx in make_list(c_idx)]) for c_idx in self.collapse_ind ]\n","\n","        for idx, (crp, x) in enumerate(zip(self.crp_blocks, c_xs)):\n","            if idx == 0:\n","                y = self.relu(x)\n","            else:\n","                y = self.relu(x + y)\n","            y = crp(y)\n","            if idx < (len(c_xs) - 1):\n","                y = self.adapt_convs[idx](y)\n","                y = tf.image.resize(\n","                    y,\n","                    size=c_xs[idx + 1].size()[2:],\n","                    method=ResizeMethod.BILINEAR,\n","                    align_corners=True,\n","                )\n","\n","        outs = []\n","        for head in self.heads:\n","            outs.append(head(y))\n","        return outs\n"],"metadata":{"id":"oNglBOFL_rPV","execution":{"iopub.status.busy":"2022-12-21T01:50:56.987605Z","iopub.execute_input":"2022-12-21T01:50:56.987973Z","iopub.status.idle":"2022-12-21T01:50:57.003739Z","shell.execute_reply.started":"2022-12-21T01:50:56.987943Z","shell.execute_reply":"2022-12-21T01:50:57.002467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class HydraNet(tf.keras.Model):\n","#     def __init__(self, num_classes, num_tasks) :\n","#         super(HydraNet, self).__init__()\n","        \n","#         self.num_classes = num_classes\n","#         self.num_tasks = num_tasks\n","        \n","#         self.encoder = MobileNetV2()\n","#         self.decoder = MTLWRefineNet(2,6)\n","        \n","#     def call(self, inputs) :\n","#         l3, l4, l5, l6, l7, l8 = self.encoder(inputs)\n","#         if self.num_tasks == 3 :\n","#             out_depth, out_segm, out_n = self.decoder(l3, l4, l5, l6, l7, l8)\n","#             return out_depth, out_segm, out_n\n","#         else :\n","#             out_depth, out_segm = self.decoder(l3, l4, l5, l6, l7, l8)\n","#             return out_depth, out_segm"],"metadata":{"id":"JSJ8T01sPmxA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class HydraNet(tf.keras.Model):\n","    def __init__(self, input_sizes, num_classes) :\n","        super(HydraNet, self).__init__()\n","        \n","        self.input_sizes = input_sizes\n","        self.num_classes = num_classes\n","        \n","        self.encoder = MobileNetV2()\n","        self.decoder = MTLWRefineNet(self.input_sizes, self.num_classes)\n","        \n","    def call(self, inputs) :\n","        l3, l4, l5, l6, l7, l8 = self.encoder(inputs)\n","        if self.num_tasks == 3 :\n","            out_depth, out_segm, out_n = self.decoder(l3, l4, l5, l6, l7, l8)\n","            return out_depth, out_segm, out_n\n","        else :\n","            out_depth, out_segm = self.decoder(l3, l4, l5, l6, l7, l8)\n","            return out_depth, out_segm"],"metadata":{"id":"Fdx-KS66B-X0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hydranet = HydraNet(40,1)"],"metadata":{"id":"TMZOWAdKHCeG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # num_classes = (40, 1)\n","# # decoder = MTLWRefineNet(encoder._out_c, num_classes)\n","# # print(decoder)\n","# hydranet = HydraNet(40, 1)"],"metadata":{"id":"mf_5m6FGpXRR","execution":{"iopub.status.busy":"2022-12-21T01:52:28.266878Z","iopub.execute_input":"2022-12-21T01:52:28.268247Z","iopub.status.idle":"2022-12-21T01:52:28.286571Z","shell.execute_reply.started":"2022-12-21T01:52:28.268193Z","shell.execute_reply":"2022-12-21T01:52:28.285506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3 ‚Äî Train the Model\n","\n","Now that we've define our encoder and decoder. We are ready to train our model on the NYUDv2 Dataset.\n","\n","Here's what we'll need:\n","\n","*   Functions like **train() and valid()**\n","*   **An Optimizer and a Loss Function**\n","*   **Hyperparameters** such as Weight Decay, Momentum, Learning Rate, Epochs, ...\n","\n","Doesn't sound so bad, does it?"],"metadata":{"id":"-gHOn86dCbJQ"}},{"cell_type":"markdown","source":["## Loss Function\n","\n","Let's begin with the Loss and Optimization we'll need.\n","\n","* The **Segmentation Loss** is the **Cross Entropy Loss**, working as a per-pixel classification function with 15 or so classes.\n","\n","* The **Depth Loss** will be the **Inverse Huber Loss**."],"metadata":{"id":"Mh3thOQ9tIH3"}},{"cell_type":"code","source":["from utils import InvHuberLoss\n","\n","ignore_index = 255\n","ignore_depth = 0\n","\n","crit_segm = tf.keras.losses.SparseCategoricalCrossentropy()#TODO: Define the Loss for Segmentation\n","crit_depth = tf.keras.losses.Huber() #TODO: Define the Loss for Depth"],"metadata":{"id":"NZwy2ATBtHQf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Optimizer\n","For the optimizer, we'll use the **Stochastic Gradient Descent**. We'll also add techniques such as weight decay or momentum."],"metadata":{"id":"Und9VjEtt73H"}},{"cell_type":"code","source":["lr_encoder = 1e-2\n","lr_decoder = 1e-3\n","momentum_encoder = 0.9\n","momentum_decoder = 0.9\n","weight_decay_encoder = 1e-5\n","weight_decay_decoder = 1e-5"],"metadata":{"id":"pKEiyVXGt1To"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optims = [tf.keras.optimizers.experimental.SGD(learning_rate=lr_encoder, momentum=momentum_encoder, weight_decay=weight_decay_encoder),\n","         tf.keras.optimizers.experimental.SGD(learning_rate=lr_decoder, momentum=momentum_decoder, weight_decay=weight_decay_decoder)]"],"metadata":{"id":"31Imhyw6t1az"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Definition & State Loading"],"metadata":{"id":"RuueL8XVvaFN"}},{"cell_type":"code","source":["n_epochs = 1000"],"metadata":{"id":"32DXd6vfvcIL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"tnsOBpwIGGTx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from model_helpers import Saver, load_state_dict\n","import operator \n","import json\n","import logging\n","\n","init_vals = (0.0, 10000.0)\n","comp_fns = [operator.gt, operator.lt]\n","ckpt_dir = \"./\"\n","ckpt_path = \"./checkpoint.pth.tar\"\n","\n","saver = Saver(\n","    args=locals(),\n","    ckpt_dir=ckpt_dir,\n","    best_val=init_vals,\n","    condition=comp_fns,\n","    save_several_mode=all,\n",")"],"metadata":{"id":"0aelXUe9wtbg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hydranet = nn.DataParallel(nn.Sequential(encoder, decoder).cuda()) # Use .cpu() if you prefer a slow death\n","\n","print(\"Model has {} parameters\".format(sum([p.numel() for p in hydranet.parameters()])))\n","\n","start_epoch, _, state_dict = saver.maybe_load(ckpt_path=ckpt_path, keys_to_load=[\"epoch\", \"best_val\", \"state_dict\"],)\n","load_state_dict(hydranet, state_dict)\n","\n","if start_epoch is None:\n","    start_epoch = 0"],"metadata":{"id":"sJjsbuthxPwB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(start_epoch)"],"metadata":{"id":"ayNsDMhax-cp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Learning Rate Scheduler"],"metadata":{"id":"mre_kFpoxPMA"}},{"cell_type":"code","source":["opt_scheds = []\n","for opt in optims:\n","    opt_scheds.append(torch.optim.lr_scheduler.MultiStepLR(opt, np.arange(start_epoch + 1, n_epochs, 100), gamma=0.1))"],"metadata":{"id":"mEze71T0wQR9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training and Validation Loops\n","\n","Now, all we need to do is go through the Train and Validation DataLoaders, and train our model.\n","\n","It should look like this:\n","```python\n","for i in range(start_epoch, n_epochs):\n","    for sched in opt_scheds:\n","        sched.step(i)\n","    hydranet.train() # Set to train mode    \n","    train(...) # Call the train function\n","\n","    if i % val_every == 0:\n","        model1.eval() # Set to Eval Mode\n","        with torch.no_grad():\n","            vals = validate(...) # Call the validate function\n","```\n","\n","In the (...), we'll send our dataloader, loss functions, optimizers, and everything we've defined before.<p>\n","\n","Which means **we need a training and validate functions.**"],"metadata":{"id":"NvPeyUELwe8e"}},{"cell_type":"code","source":["from utils import AverageMeter\n","from tqdm import tqdm"],"metadata":{"id":"XmAvw7eY0sQu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, opts, crits, dataloader, loss_coeffs=(1.0,), grad_norm=0.0):\n","    model.train()\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    loss_meter = AverageMeter()\n","    pbar = tqdm(dataloader)\n","\n","    for sample in pbar:\n","        loss = 0.0\n","        input = #TODO: Get the Input\n","        targets = #TODO: Get the Targets\n","        \n","        #FORWARD\n","        outputs = #TODO: Run a Forward pass\n","\n","        for out, target, crit, loss_coeff in zip(outputs, targets, crits, loss_coeffs):\n","            #TODO: Increment the Loss\n","\n","        # BACKWARD\n","        #TODO: Zero Out the Gradients\n","        #TODO: Call Loss.Backward\n","\n","        if grad_norm > 0.0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm)\n","        #TODO: Run one step\n","\n","        loss_meter.update(loss.item())\n","        pbar.set_description(\n","            \"Loss {:.3f} | Avg. Loss {:.3f}\".format(loss.item(), loss_meter.avg)\n","        )"],"metadata":{"id":"fI30C_Cqzc4G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate(model, metrics, dataloader):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.eval()\n","    for metric in metrics:\n","        metric.reset()\n","\n","    pbar = tqdm(dataloader)\n","\n","    def get_val(metrics):\n","        results = [(m.name, m.val()) for m in metrics]\n","        names, vals = list(zip(*results))\n","        out = [\"{} : {:4f}\".format(name, val) for name, val in results]\n","        return vals, \" | \".join(out)\n","\n","    with torch.no_grad():\n","        for sample in pbar:\n","            # Get the Data\n","            input = sample[\"image\"].float().to(device)\n","            targets = [sample[k].to(device) for k in dataloader.dataset.masks_names]\n","\n","            #input, targets = get_input_and_targets(sample=sample, dataloader=dataloader, device=device)\n","            targets = [target.squeeze(dim=1).cpu().numpy() for target in targets]\n","\n","            # Forward\n","            outputs = model(input)\n","            #outputs = make_list(outputs)\n","\n","            # Backward\n","            for out, target, metric in zip(outputs, targets, metrics):\n","                metric.update(\n","                    F.interpolate(out, size=target.shape[1:], mode=\"bilinear\", align_corners=False)\n","                    .squeeze(dim=1)\n","                    .cpu()\n","                    .numpy(),\n","                    target,\n","                )\n","            pbar.set_description(get_val(metrics)[1])\n","    vals, _ = get_val(metrics)\n","    print(\"----\" * 5)\n","    return vals"],"metadata":{"id":"qo5DK2vH0vcK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Main Loop"],"metadata":{"id":"YtPmGx3j2VKk"}},{"cell_type":"code","source":["from utils import MeanIoU, RMSE"],"metadata":{"id":"5FpRg81N205P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["crop_size = 400\n","batch_size = 4\n","val_batch_size = 4\n","val_every = 5\n","loss_coeffs = (0.5, 0.5)\n","\n","#TODO: Define a Training Loop! (Good Luck!)"],"metadata":{"id":"ozf0W-0I1yVc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference Challenge\n","\n","Now that your model is trained and checkpoint saved, try and **load an image from the test dataset and run your model on it**. Print the FPS.\n","<p>\n","\n","**MEGA POINTS** ‚Äî Load a video, and **implement a video pipeline** as we did on the previous workshop!"],"metadata":{"id":"1MCe8myT4wtk"}},{"cell_type":"code","source":["#Good Luck! If you have any good result, send it to jeremy@thinkautonomous.ai directly!"],"metadata":{"id":"7PR-brzyAzIH"},"execution_count":null,"outputs":[]}]}